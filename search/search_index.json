{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>NV-Ingest is a scalable, performance-oriented document content and metadata extraction microservice. NV-Ingest uses specialized NVIDIA NIM microservices to find, contextualize, and extract text, tables, charts and images for use in downstream generative applications.. You can access NV-Ingest as a free community resource or learn more about getting an enterprise license for improved expert-level support at the NV-Ingest homepage.</p> <ul> <li> <p> User Guide</p> <p>Install NV-Ingest and set up your environment to start accelerating your workflows.</p> <p>Get Started</p> </li> </ul>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>User Guide</li> </ul>"},{"location":"user-guide/","title":"What is NV-Ingest?","text":"<p>NV-Ingest is a scalable, performance-oriented document content and metadata extraction microservice.  NV-Ingest uses specialized NVIDIA NIM microservices  to find, contextualize, and extract text, tables, charts and images that you can use in downstream generative applications.</p> <p>NV-Ingest also enables parallelization of the process of splitting documents into pages where contents are classified (such as tables, charts, images, text),  extracted into discrete content, and further contextualized through optical character recognition (OCR) into a well defined JSON schema.  From there, NVIDIA-Ingest can optionally manage computation of embeddings for the extracted content,  and optionally manage storing into a vector database Milvus.</p>"},{"location":"user-guide/#what-nv-ingest-is","title":"What NV-Ingest Is \u2714\ufe0f","text":"<p>NV-Ingest is a microservice service that does the following:</p> <ul> <li>Accepts a JSON Job description, containing a document payload, and a set of ingestion tasks to perform on that payload.</li> <li>Allows the results of a Job to be retrieved; the result is a JSON dictionary containing a list of Metadata describing objects extracted from the base document, and processing annotations and timing/trace data.</li> <li>Supports .pdf, .docx, .pptx, and images.</li> <li>Supports multiple methods of extraction for each document type to balance trade-offs between throughput and accuracy. For example, for PDF documents, we support extraction through pdfium, Unstructured.io, and Adobe Content Extraction Services.</li> <li>Supports various types of pre and post processing operations, including text splitting and chunking, transform and filtering, embedding generation, and image offloading to storage.</li> </ul>"},{"location":"user-guide/#what-nv-ingest-isnt","title":"What NV-Ingest Isn't \u2716\ufe0f","text":"<p>NV-Ingest does not do the following:</p> <ul> <li>Runs a static pipeline or fixed set of operations on every submitted document.</li> <li>Acts as a wrapper for any specific document parsing library.</li> </ul>"},{"location":"user-guide/SUMMARY/","title":"SUMMARY","text":"<ul> <li>What is NVIDIA Ingest?</li> <li>Getting Started</li> <li>Developer Guide</li> <li>Contributing</li> <li>API Reference</li> <li>Appendix</li> </ul>"},{"location":"user-guide/api/nv_ingest_api/","title":"nv-ingest-api","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim","title":"<code>nim</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached","title":"<code>cached</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface","title":"<code>CachedModelInterface</code>","text":"<p>               Bases: <code>ModelInterface</code></p> <p>An interface for handling inference with a Cached model, supporting both gRPC and HTTP protocols, including batched input.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface._extract_content_from_nim_response","title":"<code>_extract_content_from_nim_response(json_response)</code>","text":"<p>Extract content from the JSON response of a NIM (HTTP) API request.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface._extract_content_from_nim_response--parameters","title":"Parameters","text":"<p>json_response : dict of str -&gt; Any     The JSON response from the NIM API.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface._extract_content_from_nim_response--returns","title":"Returns","text":"<p>Any     The extracted content from the response.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface._extract_content_from_nim_response--raises","title":"Raises","text":"<p>RuntimeError     If the response format is unexpected (missing 'data' or empty).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.format_input","title":"<code>format_input(data, protocol, max_batch_size, **kwargs)</code>","text":"<p>Format input data for the specified protocol (\"grpc\" or \"http\"), handling batched images.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.format_input--parameters","title":"Parameters","text":"<p>data : dict of str -&gt; Any     The input data dictionary, expected to contain \"image_arrays\" (a list of np.ndarray). protocol : str     The protocol to use, \"grpc\" or \"http\". max_batch_size : int     The maximum number of images per batch.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.format_input--returns","title":"Returns","text":"<p>Any     A list of formatted input batches. For gRPC, each batch is a NumPy array of shape (B, H, W, C)     where B &lt;= max_batch_size. For HTTP, each batch is a JSON-serializable dict containing base64\u2011encoded     images.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.format_input--raises","title":"Raises","text":"<p>KeyError     If \"image_arrays\" is missing in the data dictionary. ValueError     If the protocol is invalid, or if images have differing shapes for gRPC.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.name","title":"<code>name()</code>","text":"<p>Get the name of the model interface.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.name--returns","title":"Returns","text":"<p>str     The name of the model interface (\"Cached\").</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.parse_output","title":"<code>parse_output(response, protocol, data=None, **kwargs)</code>","text":"<p>Parse the output from the Cached model's inference response.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.parse_output--parameters","title":"Parameters","text":"<p>response : Any     The raw response from the model inference. protocol : str     The protocol used (\"grpc\" or \"http\"). data : dict of str -&gt; Any, optional     Additional input data (unused here, but available for consistency). **kwargs : Any     Additional keyword arguments for future compatibility.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.parse_output--returns","title":"Returns","text":"<p>Any     The parsed output data (e.g., list of strings), depending on the protocol.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.parse_output--raises","title":"Raises","text":"<p>ValueError     If the protocol is invalid. RuntimeError     If the HTTP response is not as expected (missing 'data' key).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.prepare_data_for_inference","title":"<code>prepare_data_for_inference(data)</code>","text":"<p>Decode base64-encoded images into NumPy arrays, storing them in <code>data[\"image_arrays\"]</code>.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.prepare_data_for_inference--parameters","title":"Parameters","text":"<p>data : dict of str -&gt; Any     The input data containing either:      - \"base64_image\": a single base64-encoded image, or      - \"base64_images\": a list of base64-encoded images.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.prepare_data_for_inference--returns","title":"Returns","text":"<p>dict of str -&gt; Any     The updated data dictionary with decoded image arrays stored in     \"image_arrays\", where each array has shape (H, W, C).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.prepare_data_for_inference--raises","title":"Raises","text":"<p>KeyError     If neither 'base64_image' nor 'base64_images' is provided. ValueError     If 'base64_images' is provided but is not a list.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.process_inference_results","title":"<code>process_inference_results(output, protocol, **kwargs)</code>","text":"<p>Process inference results for the Cached model.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.process_inference_results--parameters","title":"Parameters","text":"<p>output : Any     The raw output from the model. protocol : str     The inference protocol used (\"grpc\" or \"http\"). **kwargs : Any     Additional parameters for post-processing (not used here).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.cached.CachedModelInterface.process_inference_results--returns","title":"Returns","text":"<p>Any     The processed inference results, which here is simply returned as-is.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.decorators","title":"<code>decorators</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.decorators.global_cache","title":"<code>global_cache = manager.dict()</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.decorators.lock","title":"<code>lock = Lock()</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.decorators.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.decorators.manager","title":"<code>manager = Manager()</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.decorators.multiprocessing_cache","title":"<code>multiprocessing_cache(max_calls)</code>","text":"<p>A decorator that creates a global cache shared between multiple processes. The cache is invalidated after <code>max_calls</code> number of accesses.</p> <p>Parameters:</p> Name Type Description Default <code>max_calls</code> <code>int</code> <p>The number of calls after which the cache is cleared.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>The decorated function with global cache and invalidation logic.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot","title":"<code>deplot</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface","title":"<code>DeplotModelInterface</code>","text":"<p>               Bases: <code>ModelInterface</code></p> <p>An interface for handling inference with a Deplot model, supporting both gRPC and HTTP protocols, now updated to handle multiple base64 images ('base64_images').</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface._extract_content_from_deplot_response","title":"<code>_extract_content_from_deplot_response(json_response)</code>  <code>staticmethod</code>","text":"<p>Extract content from the JSON response of a Deplot HTTP API request. The original code expected a single choice with a single textual content.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface._prepare_deplot_payload","title":"<code>_prepare_deplot_payload(base64_list, max_tokens=500, temperature=0.5, top_p=0.9)</code>  <code>staticmethod</code>","text":"<p>Prepare an HTTP payload for Deplot that includes one message per image, matching the original single-image style:</p> <pre><code>messages = [\n  {\n    \"role\": \"user\",\n    \"content\": \"Generate ... &lt;img src=\"data:image/png;base64,...\" /&gt;\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"Generate ... &lt;img src=\"data:image/png;base64,...\" /&gt;\"\n  },\n  ...\n]\n</code></pre> <p>If your backend expects multiple messages in a single request, this keeps the same structure as the single-image code repeated N times.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.format_input","title":"<code>format_input(data, protocol, max_batch_size, **kwargs)</code>","text":"<p>Format input data for the specified protocol (gRPC or HTTP). For HTTP, we now construct multiple messages\u2014one per image batch\u2014in the same style as the original single-image code.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.format_input--parameters","title":"Parameters","text":"<p>data : dict of str -&gt; Any     The input data dictionary, expected to contain \"image_arrays\" (a list of np.ndarray). protocol : str     The protocol to use, \"grpc\" or \"http\". max_batch_size : int     The maximum number of images per batch. kwargs : dict     Additional parameters to pass to the payload preparation (for HTTP).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.format_input--returns","title":"Returns","text":"<p>Any     For gRPC: A list of NumPy arrays, each of shape (B, H, W, C) with B &lt;= max_batch_size.     For HTTP: A list of JSON-serializable payload dicts, each containing up to max_batch_size images.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.format_input--raises","title":"Raises","text":"<p>KeyError     If \"image_arrays\" is missing in the data dictionary. ValueError     If the protocol is invalid, or if images have differing shapes for gRPC.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.name","title":"<code>name()</code>","text":"<p>Get the name of the model interface.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.name--returns","title":"Returns","text":"<p>str     The name of the model interface (\"Deplot\").</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.parse_output","title":"<code>parse_output(response, protocol, data=None, **kwargs)</code>","text":"<p>Parse the model's inference response.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.prepare_data_for_inference","title":"<code>prepare_data_for_inference(data)</code>","text":"<p>Prepare input data by decoding one or more base64-encoded images into NumPy arrays.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.prepare_data_for_inference--parameters","title":"Parameters","text":"<p>data : dict     The input data containing either 'base64_image' (single image)     or 'base64_images' (multiple images).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.prepare_data_for_inference--returns","title":"Returns","text":"<p>dict     The updated data dictionary with 'image_arrays': a list of decoded NumPy arrays.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.process_inference_results","title":"<code>process_inference_results(output, protocol, **kwargs)</code>","text":"<p>Process inference results for the Deplot model.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.process_inference_results--parameters","title":"Parameters","text":"<p>output : Any     The raw output from the model. protocol : str     The protocol used for inference (gRPC or HTTP).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.deplot.DeplotModelInterface.process_inference_results--returns","title":"Returns","text":"<p>Any     The processed inference results.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut","title":"<code>doughnut</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.ACCEPTED_CLASSES","title":"<code>ACCEPTED_CLASSES = ACCEPTED_TEXT_CLASSES | ACCEPTED_TABLE_CLASSES | ACCEPTED_IMAGE_CLASSES</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.ACCEPTED_IMAGE_CLASSES","title":"<code>ACCEPTED_IMAGE_CLASSES = set(['Picture'])</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.ACCEPTED_TABLE_CLASSES","title":"<code>ACCEPTED_TABLE_CLASSES = set(['Table'])</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.ACCEPTED_TEXT_CLASSES","title":"<code>ACCEPTED_TEXT_CLASSES = set(['Text', 'Title', 'Section-header', 'List-item', 'TOC', 'Bibliography', 'Formula', 'Page-header', 'Page-footer', 'Caption', 'Footnote', 'Floating-text'])</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut._re_extract_class_bbox","title":"<code>_re_extract_class_bbox = re.compile('&lt;x_(\\\\d+)&gt;&lt;y_(\\\\d+)&gt;((?:|.(?:(?&lt;!&lt;x_\\\\d)(?&lt;!&lt;y_\\\\d)(?&lt;!&lt;class_[A-Za-z0-9]).)*))&lt;x_(\\\\d+)&gt;&lt;y_(\\\\d+)&gt;&lt;class_([A-Za-z0-9\\\\-]+)&gt;', re.MULTILINE | re.DOTALL)</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut._fix_dots","title":"<code>_fix_dots(m)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.extract_classes_bboxes","title":"<code>extract_classes_bboxes(text)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.postprocess_text","title":"<code>postprocess_text(txt, cls)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.reverse_transform_bbox","title":"<code>reverse_transform_bbox(bbox, bbox_offset, original_width, original_height)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.doughnut.strip_markdown_formatting","title":"<code>strip_markdown_formatting(text)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers","title":"<code>helpers</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.DEPLOT_MAX_TOKENS","title":"<code>DEPLOT_MAX_TOKENS = 128</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.DEPLOT_TEMPERATURE","title":"<code>DEPLOT_TEMPERATURE = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.DEPLOT_TOP_P","title":"<code>DEPLOT_TOP_P = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface","title":"<code>ModelInterface</code>","text":"<p>Base class for defining a model interface that supports preparing input data, formatting it for inference, parsing output, and processing inference results.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.format_input","title":"<code>format_input(data, protocol, max_batch_size)</code>","text":"<p>Format the input data for the specified protocol.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.format_input--parameters","title":"Parameters","text":"<p>data : dict     The input data to format. protocol : str     The protocol to format the data for.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.name","title":"<code>name()</code>","text":"<p>Get the name of the model interface.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.name--returns","title":"Returns","text":"<p>str     The name of the model interface.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.parse_output","title":"<code>parse_output(response, protocol, data=None, **kwargs)</code>","text":"<p>Parse the output data from the model's inference response.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.parse_output--parameters","title":"Parameters","text":"<p>response : Any     The response from the model inference. protocol : str     The protocol used (\"grpc\" or \"http\"). data : dict, optional     Additional input data passed to the function.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.prepare_data_for_inference","title":"<code>prepare_data_for_inference(data)</code>","text":"<p>Prepare input data for inference by processing or transforming it as required.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.prepare_data_for_inference--parameters","title":"Parameters","text":"<p>data : dict     The input data to prepare.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.process_inference_results","title":"<code>process_inference_results(output_array, protocol, **kwargs)</code>","text":"<p>Process the inference results from the model.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.ModelInterface.process_inference_results--parameters","title":"Parameters","text":"<p>output_array : Any     The raw output from the model. kwargs : dict     Additional parameters for processing.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient","title":"<code>NimClient</code>","text":"<p>A client for interfacing with a model inference server using gRPC or HTTP protocols.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._lock","title":"<code>_lock = threading.Lock()</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._max_batch_sizes","title":"<code>_max_batch_sizes = {}</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.auth_token","title":"<code>auth_token = auth_token</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.client","title":"<code>client = None</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.endpoint_url","title":"<code>endpoint_url = generate_url(self._http_endpoint)</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.headers","title":"<code>headers = {'accept': 'application/json', 'content-type': 'application/json'}</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.max_retries","title":"<code>max_retries = max_retries</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.model_interface","title":"<code>model_interface = model_interface</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.protocol","title":"<code>protocol = protocol.lower()</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.timeout","title":"<code>timeout = timeout</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.__init__","title":"<code>__init__(model_interface, protocol, endpoints, auth_token=None, timeout=120.0, max_retries=5)</code>","text":"<p>Initialize the NimClient with the specified model interface, protocol, and server endpoints.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.__init__--parameters","title":"Parameters","text":"<p>model_interface : ModelInterface     The model interface implementation to use. protocol : str     The protocol to use (\"grpc\" or \"http\"). endpoints : tuple     A tuple containing the gRPC and HTTP endpoints. auth_token : str, optional     Authorization token for HTTP requests (default: None). timeout : float, optional     Timeout for HTTP requests in seconds (default: 30.0).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.__init__--raises","title":"Raises","text":"<p>ValueError     If an invalid protocol is specified or if required endpoints are missing.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._fetch_max_batch_size","title":"<code>_fetch_max_batch_size(model_name, model_version='')</code>","text":"<p>Fetch the maximum batch size from the Triton model configuration in a thread-safe manner.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._grpc_infer","title":"<code>_grpc_infer(formatted_input, model_name)</code>","text":"<p>Perform inference using the gRPC protocol.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._grpc_infer--parameters","title":"Parameters","text":"<p>formatted_input : np.ndarray     The input data formatted as a numpy array. model_name : str     The name of the model to use for inference.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._grpc_infer--returns","title":"Returns","text":"<p>np.ndarray     The output of the model as a numpy array.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._http_infer","title":"<code>_http_infer(formatted_input)</code>","text":"<p>Perform inference using the HTTP protocol, retrying for timeouts or 5xx errors up to 5 times.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._http_infer--parameters","title":"Parameters","text":"<p>formatted_input : dict     The input data formatted as a dictionary.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._http_infer--returns","title":"Returns","text":"<p>dict     The output of the model as a dictionary.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._http_infer--raises","title":"Raises","text":"<p>TimeoutError     If the HTTP request times out repeatedly, up to the max retries. requests.RequestException     For other HTTP-related errors that persist after max retries.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._process_batch","title":"<code>_process_batch(batch_input, *, prepared_data, model_name, **kwargs)</code>","text":"<p>Process a single batch input for inference.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._process_batch--parameters","title":"Parameters","text":"<p>batch_input : Any     The batch input data to process. prepared_data : Any     The prepared data used for inference. model_name : str     The model name to use for inference. kwargs : dict     Additional parameters for inference.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient._process_batch--returns","title":"Returns","text":"<p>Any     The parsed output from the inference request.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.close","title":"<code>close()</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.infer","title":"<code>infer(data, model_name, **kwargs)</code>","text":"<p>Perform inference using the specified model and input data.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.infer--parameters","title":"Parameters","text":"<p>data : dict     The input data for inference. model_name : str     The name of the model to use for inference. kwargs : dict     Additional parameters for inference. Optionally supports \"max_pool_workers\" to set     the number of worker threads in the thread pool.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.infer--returns","title":"Returns","text":"<p>Any     The processed inference results.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.infer--raises","title":"Raises","text":"<p>ValueError     If an invalid protocol is specified.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.NimClient.try_set_max_batch_size","title":"<code>try_set_max_batch_size(model_name, model_version='')</code>","text":"<p>Attempt to set the max batch size for the model if it is not already set, ensuring thread safety.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.create_inference_client","title":"<code>create_inference_client(endpoints, model_interface, auth_token=None, infer_protocol=None)</code>","text":"<p>Create a NimClient for interfacing with a model inference server.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.create_inference_client--parameters","title":"Parameters","text":"<p>endpoints : tuple     A tuple containing the gRPC and HTTP endpoints. model_interface : ModelInterface     The model interface implementation to use. auth_token : str, optional     Authorization token for HTTP requests (default: None). infer_protocol : str, optional     The protocol to use (\"grpc\" or \"http\"). If not specified, it is inferred from the endpoints.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.create_inference_client--returns","title":"Returns","text":"<p>NimClient     The initialized NimClient.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.create_inference_client--raises","title":"Raises","text":"<p>ValueError     If an invalid infer_protocol is specified.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.generate_url","title":"<code>generate_url(url)</code>","text":"<p>Examines the user defined URL for http*://. If that pattern is detected the URL is used as provided by the user. If that pattern does not exist then the assumption is made that the endpoint is simply <code>http://</code> and that is prepended to the user supplied endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Endpoint where the Rest service is running</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Fully validated URL</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.get_version","title":"<code>get_version(http_endpoint, metadata_endpoint='/v1/metadata', version_field='version')</code>","text":"<p>Get the version of the server from its metadata endpoint.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.get_version--parameters","title":"Parameters","text":"<p>http_endpoint : str     The HTTP endpoint of the server. metadata_endpoint : str, optional     The metadata endpoint to query (default: \"/v1/metadata\"). version_field : str, optional     The field containing the version in the response (default: \"version\").</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.get_version--returns","title":"Returns","text":"<p>str     The version of the server, or an empty string if unavailable.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.is_ready","title":"<code>is_ready(http_endpoint, ready_endpoint)</code>","text":"<p>Check if the server at the given endpoint is ready.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.is_ready--parameters","title":"Parameters","text":"<p>http_endpoint : str     The HTTP endpoint of the server. ready_endpoint : str     The specific ready-check endpoint.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.is_ready--returns","title":"Returns","text":"<p>bool     True if the server is ready, False otherwise.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.preprocess_image_for_paddle","title":"<code>preprocess_image_for_paddle(array, paddle_version=None)</code>","text":"<p>Preprocesses an input image to be suitable for use with PaddleOCR by resizing, normalizing, padding, and transposing it into the required format.</p> <p>This function is intended for preprocessing images to be passed as input to PaddleOCR using GRPC. It is not necessary when using the HTTP endpoint.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.preprocess_image_for_paddle--steps","title":"Steps:","text":"<ol> <li>Resizes the image while maintaining aspect ratio such that its largest dimension is scaled to 960 pixels.</li> <li>Normalizes the image using the <code>normalize_image</code> function.</li> <li>Pads the image to ensure both its height and width are multiples of 32, as required by PaddleOCR.</li> <li>Transposes the image from (height, width, channel) to (channel, height, width), the format expected by PaddleOCR.</li> </ol>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.preprocess_image_for_paddle--parameters","title":"Parameters:","text":"<p>array : np.ndarray     The input image array of shape (height, width, channels). It should have pixel values in the range [0, 255].</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.preprocess_image_for_paddle--returns","title":"Returns:","text":"<p>np.ndarray     A preprocessed image with the shape (channels, height, width) and normalized pixel values.     The image will be padded to have dimensions that are multiples of 32, with the padding color set to 0.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.preprocess_image_for_paddle--notes","title":"Notes:","text":"<ul> <li>The image is resized so that its largest dimension becomes 960 pixels, maintaining the aspect ratio.</li> <li>After normalization, the image is padded to the nearest multiple of 32 in both dimensions, which is   a requirement for PaddleOCR.</li> <li>The normalized pixel values are scaled between 0 and 1 before padding and transposing the image.</li> </ul>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.helpers.remove_url_endpoints","title":"<code>remove_url_endpoints(url)</code>","text":"<p>Some configurations provide the full endpoint in the URL. Ex: http://deplot:8000/v1/chat/completions. For hitting the health endpoint we need to get just the hostname:port combo that we can append the health/ready endpoint to so we attempt to parse that information here.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Incoming URL</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>URL with just the hostname:port portion remaining</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle","title":"<code>paddle</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface","title":"<code>PaddleOCRModelInterface</code>","text":"<p>               Bases: <code>ModelInterface</code></p> <p>An interface for handling inference with a PaddleOCR model, supporting both gRPC and HTTP protocols.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.paddle_version","title":"<code>paddle_version = paddle_version</code>  <code>instance-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.__init__","title":"<code>__init__(paddle_version=None)</code>","text":"<p>Initialize the PaddleOCR model interface.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.__init__--parameters","title":"Parameters","text":"<p>paddle_version : str, optional     The version of the PaddleOCR model (default is None).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._convert_paddle_response_to_psuedo_markdown","title":"<code>_convert_paddle_response_to_psuedo_markdown(bounding_boxes, text_predictions, img_index=0, dims=None)</code>  <code>staticmethod</code>","text":"<p>Convert bounding boxes &amp; text to pseudo-markdown format. For multiple images, the correct image dimensions (height, width) are retrieved from <code>dims[img_index]</code>.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._convert_paddle_response_to_psuedo_markdown--parameters","title":"Parameters","text":"<p>bounding_boxes : list of Any     A list (per line of text) of bounding boxes, each a list of (x, y) points. text_predictions : list of str     A list of text predictions, one for each bounding box. img_index : int, optional     The index of the image for which bounding boxes are being converted. Default is 0. dims : list of (int, int), optional     A list of (height, width) for each corresponding image.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._convert_paddle_response_to_psuedo_markdown--returns","title":"Returns","text":"<p>str     The pseudo-markdown representation of detected text lines and bounding boxes.     Each cluster of text is placed on its own line, with text columns separated by '|'.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._convert_paddle_response_to_psuedo_markdown--notes","title":"Notes","text":"<ul> <li>If <code>dims</code> is None or <code>img_index</code> is out of range, bounding boxes will not be scaled properly.</li> </ul>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._extract_content_from_paddle_grpc_response","title":"<code>_extract_content_from_paddle_grpc_response(response, table_content_format, dims)</code>","text":"<p>Parse a gRPC response for one or more images. The response can have two possible shapes:   - (3,) for batch_size=1   - (3, n) for batch_size=n</p> In either case <p>response[0, i]: byte string containing bounding box data response[1, i]: byte string containing text prediction data response[2, i]: (Optional) additional data/metadata (ignored here)</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._extract_content_from_paddle_grpc_response--parameters","title":"Parameters","text":"<p>response : np.ndarray     The raw NumPy array from gRPC. Expected shape: (3,) or (3, n). table_content_format : str     The format of the output text content, e.g. 'simple' or 'pseudo_markdown'. dims : list of (int, int), optional     A list of (height, width) for each corresponding image, used for bounding box scaling.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._extract_content_from_paddle_grpc_response--returns","title":"Returns","text":"<p>list of (str, str)     A list of (content, table_content_format) for each image.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._extract_content_from_paddle_grpc_response--raises","title":"Raises","text":"<p>ValueError     If the response is not a NumPy array or has an unexpected shape,     or if the <code>table_content_format</code> is unrecognized.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._extract_content_from_paddle_http_response","title":"<code>_extract_content_from_paddle_http_response(json_response, table_content_format, dims)</code>","text":"<p>Extract content from the JSON response of a PaddleOCR HTTP API request.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._extract_content_from_paddle_http_response--parameters","title":"Parameters","text":"<p>json_response : dict of str -&gt; Any     The JSON response returned by the PaddleOCR endpoint. table_content_format : str or None     The specified format for table content (e.g., 'simple' or 'pseudo_markdown'). dims : list of (int, int), optional     A list of (height, width) for each corresponding image, used for bounding box     scaling if not None.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._extract_content_from_paddle_http_response--returns","title":"Returns","text":"<p>list of (str, str)     A list of (content, table_content_format) tuples, one for each image result.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._extract_content_from_paddle_http_response--raises","title":"Raises","text":"<p>RuntimeError     If the response format is missing or invalid. ValueError     If the <code>table_content_format</code> is unrecognized.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._is_version_early_access_legacy_api","title":"<code>_is_version_early_access_legacy_api()</code>","text":"<p>Determine if the current PaddleOCR version is considered \"early access\" and thus uses the legacy API format.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._is_version_early_access_legacy_api--returns","title":"Returns","text":"<p>bool     True if the version is &lt; 0.2.1-rc2; False otherwise.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._prepare_paddle_payload","title":"<code>_prepare_paddle_payload(base64_img)</code>","text":"<p>DEPRECATED by batch logic in format_input. Kept here if you need single-image direct calls.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._prepare_paddle_payload--parameters","title":"Parameters","text":"<p>base64_img : str     A single base64-encoded image string.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface._prepare_paddle_payload--returns","title":"Returns","text":"<p>dict of str -&gt; Any     The payload in either legacy or new format for PaddleOCR's HTTP endpoint.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.format_input","title":"<code>format_input(data, protocol, max_batch_size, **kwargs)</code>","text":"<p>Format input data for the specified protocol (\"grpc\" or \"http\"), supporting batched data.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.format_input--parameters","title":"Parameters","text":"<p>data : dict of str -&gt; Any     The input data dictionary, expected to contain \"image_arrays\" (list of np.ndarray). protocol : str     The inference protocol, either \"grpc\" or \"http\". max_batch_size : int     The maximum batch size batching.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.format_input--returns","title":"Returns","text":"<p>Any     A list of formatted batches. For gRPC, each item is a batched NumPy array of shape (B, H, W, C)     where B &lt;= max_batch_size. For HTTP, each item is a JSON-serializable payload containing the     base64 images in the format required by the PaddleOCR endpoint.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.format_input--raises","title":"Raises","text":"<p>KeyError     If \"image_arrays\" is not found in <code>data</code>. ValueError     If an invalid protocol is specified, or if the image shapes are inconsistent for gRPC batching.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.name","title":"<code>name()</code>","text":"<p>Get the name of the model interface.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.name--returns","title":"Returns","text":"<p>str     The name of the model interface, including the PaddleOCR version.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.parse_output","title":"<code>parse_output(response, protocol, data=None, **kwargs)</code>","text":"<p>Parse the model's inference response for the given protocol. The parsing may handle batched outputs for multiple images.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.parse_output--parameters","title":"Parameters","text":"<p>response : Any     The raw response from the PaddleOCR model. protocol : str     The protocol used for inference, \"grpc\" or \"http\". data : dict of str -&gt; Any, optional     Additional data dictionary that may include \"image_dims\" for bounding box scaling. **kwargs : Any     Additional keyword arguments, such as custom <code>table_content_format</code>.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.parse_output--returns","title":"Returns","text":"<p>Any     The parsed output, typically a list of (content, table_content_format) tuples.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.parse_output--raises","title":"Raises","text":"<p>ValueError     If an invalid protocol is specified.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.prepare_data_for_inference","title":"<code>prepare_data_for_inference(data)</code>","text":"<p>Decode one or more base64-encoded images into NumPy arrays, storing them alongside their dimensions in <code>data</code>.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.prepare_data_for_inference--parameters","title":"Parameters","text":"<p>data : dict of str -&gt; Any     The input data containing either:      - 'base64_image': a single base64-encoded image, or      - 'base64_images': a list of base64-encoded images.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.prepare_data_for_inference--returns","title":"Returns","text":"<p>dict of str -&gt; Any     The updated data dictionary with the following keys added:     - \"image_arrays\": List of decoded NumPy arrays of shape (H, W, C).     - \"image_dims\": List of (height, width) tuples for each decoded image.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.prepare_data_for_inference--raises","title":"Raises","text":"<p>KeyError     If neither 'base64_image' nor 'base64_images' is found in <code>data</code>. ValueError     If 'base64_images' is present but is not a list.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.process_inference_results","title":"<code>process_inference_results(output, **kwargs)</code>","text":"<p>Process inference results for the PaddleOCR model.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.process_inference_results--parameters","title":"Parameters","text":"<p>output : Any     The raw output parsed from the PaddleOCR model. **kwargs : Any     Additional keyword arguments for customization.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.paddle.PaddleOCRModelInterface.process_inference_results--returns","title":"Returns","text":"<p>Any     The post-processed inference results. By default, this simply returns the output     as the table content (or content list).</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox","title":"<code>yolox</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_CONF_THRESHOLD","title":"<code>YOLOX_CONF_THRESHOLD = 0.01</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_FINAL_SCORE","title":"<code>YOLOX_FINAL_SCORE = 0.48</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_IMAGE_PREPROC_HEIGHT","title":"<code>YOLOX_IMAGE_PREPROC_HEIGHT = 1024</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_IMAGE_PREPROC_WIDTH","title":"<code>YOLOX_IMAGE_PREPROC_WIDTH = 1024</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_IOU_THRESHOLD","title":"<code>YOLOX_IOU_THRESHOLD = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_MAX_BATCH_SIZE","title":"<code>YOLOX_MAX_BATCH_SIZE = 8</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_MAX_HEIGHT","title":"<code>YOLOX_MAX_HEIGHT = 1536</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_MAX_WIDTH","title":"<code>YOLOX_MAX_WIDTH = 1536</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_MIN_SCORE","title":"<code>YOLOX_MIN_SCORE = 0.1</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_NIM_MAX_IMAGE_SIZE","title":"<code>YOLOX_NIM_MAX_IMAGE_SIZE = 512000</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YOLOX_NUM_CLASSES","title":"<code>YOLOX_NUM_CLASSES = 3</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface","title":"<code>YoloxPageElementsModelInterface</code>","text":"<p>               Bases: <code>ModelInterface</code></p> <p>An interface for handling inference with a Yolox object detection model, supporting both gRPC and HTTP protocols.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.format_input","title":"<code>format_input(data, protocol, max_batch_size, **kwargs)</code>","text":"<p>Format input data for the specified protocol, returning a list of batches each up to 'max_batch_size' in length.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.format_input--parameters","title":"Parameters","text":"<p>data : dict     The input data to format. protocol : str     The protocol to use (\"grpc\" or \"http\"). max_batch_size : int     The maximum batch size to respect.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.format_input--returns","title":"Returns","text":"<p>List[Any]     A list of batches, each formatted according to the protocol.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.name","title":"<code>name()</code>","text":"<p>Returns the name of the Yolox model interface.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.name--returns","title":"Returns","text":"<p>str     The name of the model interface.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.parse_output","title":"<code>parse_output(response, protocol, data=None, **kwargs)</code>","text":"<p>Parse the output from the model's inference response.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.parse_output--parameters","title":"Parameters","text":"<p>response : Any     The response from the model inference. protocol : str     The protocol used (\"grpc\" or \"http\"). data : dict, optional     Additional input data passed to the function.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.parse_output--returns","title":"Returns","text":"<p>Any     The parsed output data.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.parse_output--raises","title":"Raises","text":"<p>ValueError     If an invalid protocol is specified or the response format is unexpected.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.prepare_data_for_inference","title":"<code>prepare_data_for_inference(data)</code>","text":"<p>Prepare input data for inference by resizing images and storing their original shapes.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.prepare_data_for_inference--parameters","title":"Parameters","text":"<p>data : dict     The input data containing a list of images.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.prepare_data_for_inference--returns","title":"Returns","text":"<p>dict     The updated data dictionary with resized images and original image shapes.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.process_inference_results","title":"<code>process_inference_results(output, protocol, **kwargs)</code>","text":"<p>Process the results of the Yolox model inference and return the final annotations.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.process_inference_results--parameters","title":"Parameters","text":"<p>output_array : np.ndarray     The raw output from the Yolox model. kwargs : dict     Additional parameters for processing, including thresholds and number of classes.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.YoloxPageElementsModelInterface.process_inference_results--returns","title":"Returns","text":"<p>list[dict]     A list of annotation dictionaries for each image in the batch.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.bb_iou_array","title":"<code>bb_iou_array(boxes, new_box)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.chunkify","title":"<code>chunkify(lst, chunk_size)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.expand_boxes","title":"<code>expand_boxes(boxes, r_x=1, r_y=1)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.expand_chart_bboxes","title":"<code>expand_chart_bboxes(annotation_dict, labels=None)</code>","text":"<p>Expand bounding boxes of charts and titles based on the bounding boxes of the other class. Args:     annotation_dict: output of postprocess_results, a dictionary with keys \"table\", \"figure\", \"title\"</p> <p>Returns:</p> Name Type Description <code>annotation_dict</code> <p>same as input, with expanded bboxes for charts</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.expand_table_bboxes","title":"<code>expand_table_bboxes(annotation_dict, labels=None)</code>","text":"<p>Additional preprocessing for tables: extend the upper bounds to capture titles if any. Args:     annotation_dict: output of postprocess_results, a dictionary with keys \"table\", \"figure\", \"title\"</p> <p>Returns:</p> Name Type Description <code>annotation_dict</code> <p>same as input, with expanded bboxes for charts</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.find_matching_box_fast","title":"<code>find_matching_box_fast(boxes_list, new_box, match_iou)</code>","text":"<p>Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays (~100x). This was previously the bottleneck since the function is called for every entry in the array.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.get_biggest_box","title":"<code>get_biggest_box(boxes, conf_type='avg')</code>","text":"<p>Merges boxes by using the biggest box.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>np array [n x 8]</code> <p>Boxes to merge.</p> required <code>conf_type</code> <code>str</code> <p>Confidence merging type. Defaults to \"avg\".</p> <code>'avg'</code> <p>Returns:</p> Type Description <p>np array [8]: Merged box.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.get_weighted_box","title":"<code>get_weighted_box(boxes, conf_type='avg')</code>","text":"<p>Merges boxes by using the weighted fusion.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>np array [n x 8]</code> <p>Boxes to merge.</p> required <code>conf_type</code> <code>str</code> <p>Confidence merging type. Defaults to \"avg\".</p> <code>'avg'</code> <p>Returns:</p> Type Description <p>np array [8]: Merged box.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.match_with_title","title":"<code>match_with_title(chart_bbox, title_bboxes, iou_th=0.01)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.merge_boxes","title":"<code>merge_boxes(b1, b2)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.merge_labels","title":"<code>merge_labels(labels, confs)</code>","text":"<p>Custom function for merging labels. If all labels are the same, return the unique value. Else, return the label of the most confident non-title (class 2) box.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>np array [n]</code> <p>Labels.</p> required <code>confs</code> <code>np array [n]</code> <p>Confidence.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>Label.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.postprocess_model_prediction","title":"<code>postprocess_model_prediction(prediction, num_classes, conf_thre=0.7, nms_thre=0.45, class_agnostic=False)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.postprocess_results","title":"<code>postprocess_results(results, original_image_shapes, min_score=0.0)</code>","text":"<p>For each item (==image) in results, computes annotations in the form</p> <p>{\"table\": [[0.0107, 0.0859, 0.7537, 0.1219, 0.9861], ...],   \"figure\": [...],   \"title\": [...]   } where each list of 5 floats represents a bounding box in the format [x1, y1, x2, y2, confidence]</p> <p>Keep only bboxes with high enough confidence.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.prefilter_boxes","title":"<code>prefilter_boxes(boxes, scores, labels, weights, thr, class_agnostic=False)</code>","text":"<p>Reformats and filters boxes. Output is a dict of boxes to merge separately.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>list[np array[n x 4]]</code> <p>List of boxes. One list per model.</p> required <code>scores</code> <code>list[np array[n]]</code> <p>List of confidences.</p> required <code>labels</code> <code>list[np array[n]]</code> <p>List of labels.</p> required <code>weights</code> <code>list</code> <p>Model weights.</p> required <code>thr</code> <code>float</code> <p>Confidence threshold</p> required <code>class_agnostic</code> <code>bool</code> <p>If True, merge boxes from different classes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>dict[np array [? x 8]]: Filtered boxes.</p>"},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.resize_image","title":"<code>resize_image(image, target_img_size)</code>","text":""},{"location":"user-guide/api/nv_ingest_api/#nv_ingest_api.nim.yolox.weighted_boxes_fusion","title":"<code>weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=0.5, skip_box_thr=0.0, conf_type='avg', merge_type='weighted', class_agnostic=False)</code>","text":"<p>Custom wbf implementation that supports a class_agnostic mode and a biggest box fusion. Boxes are expected to be in normalized (x0, y0, x1, y1) format.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_list</code> <code>list[np array[n x 4]]</code> <p>List of boxes. One list per model.</p> required <code>scores_list</code> <code>list[np array[n]]</code> <p>List of confidences.</p> required <code>labels_list</code> <code>list[np array[n]]</code> <p>List of labels</p> required <code>iou_thr</code> <code>float</code> <p>IoU threshold for matching. Defaults to 0.55.</p> <code>0.5</code> <code>skip_box_thr</code> <code>float</code> <p>Exclude boxes with score &lt; skip_box_thr. Defaults to 0.0.</p> <code>0.0</code> <code>conf_type</code> <code>str</code> <p>Confidence merging type. Defaults to \"avg\".</p> <code>'avg'</code> <code>merge_type</code> <code>str</code> <p>Merge type \"weighted\" or \"biggest\". Defaults to \"weighted\".</p> <code>'weighted'</code> <code>class_agnostic</code> <code>bool</code> <p>If True, merge boxes from different classes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>np array[N x 4]: Merged boxes,</p> <p>np array[N]: Merged confidences,</p> <p>np array[N]: Merged labels.</p>"},{"location":"user-guide/api/nv_ingest_api/#nim","title":"Nim","text":""},{"location":"user-guide/appendix/releasenotes-nv-ingest/","title":"NVIDIA-Ingest Release Notes","text":""},{"location":"user-guide/appendix/releasenotes-nv-ingest/#release-2412","title":"Release 24.12","text":""},{"location":"user-guide/appendix/releasenotes-nv-ingest/#known-issues","title":"Known Issues","text":"<p>We currently do not support OCR-based text extraction. This was discovered in an unsupported use case and is not a product functionality issue.</p>"},{"location":"user-guide/contributing/code-review/","title":"Code Review","text":"<p>This document describes the process and etiquette for code review your lib</p>"},{"location":"user-guide/contributing/contributing/","title":"Contributing Guidelines","text":"<p>Note</p> <p>For code review standards please see the Code Review page.</p> <pre><code>For all PRs, an approved NVIDIA staff member must sign off and trigger the continuous integration (CI) tests.\nThese are initiated by the member commenting `/build-ci` directly on the PR. All PRs must have successful CI runs and\nsufficient code review before being merged.\n</code></pre>"},{"location":"user-guide/contributing/contributing/#python-coding-standards","title":"Python Coding Standards","text":"<p>This page contains the Python coding standards...</p>"},{"location":"user-guide/contributing/Writing%20Documentation/","title":"Writing Good and Thorough Documentation","text":"<p>As a contributor to our codebase, writing high-quality documentation is an essential part of ensuring that others can understand and work with your code effectively. Good documentation helps to reduce confusion, facilitate collaboration, and streamline the development process. In this guide, we will outline the principles and best practices for writing thorough and readable documentation that adheres to the Chicago Manual of Style.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/#chicago-manual-of-style","title":"Chicago Manual of Style","text":"<p>Our documentation follows the Chicago Manual of Style, a widely accepted standard for writing and formatting. This style guide provides a consistent approach to writing, grammar, and punctuation, making it easier for readers to understand and navigate our documentation.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/#key-principles","title":"Key Principles","text":"<p>When writing documentation, keep the following principles in mind:</p> <ol> <li>Clarity: Use clear and concise language to convey your message. Avoid ambiguity and jargon that may confuse readers.</li> <li>Accuracy: Ensure that your documentation is accurate and up-to-date. Verify facts, details, and code snippets     before publishing.</li> <li>Completeness: Provide all necessary information to understand the code, including context, syntax, and examples.</li> <li>Consistency: Use a consistent tone, voice, and style throughout the documentation.</li> <li>Accessibility: Make your documentation easy to read and understand by using headings, bullet points, and short paragraphs.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#documentation-structure","title":"Documentation Structure","text":"<p>A well-structured documentation page should include the following elements:</p> <ol> <li>Header: A brief title that summarizes the content of the page.</li> <li>Introduction: A short overview of the topic, including its purpose and relevance.</li> <li>Syntax and Parameters: A detailed explanation of the code syntax, including parameters, data types, and return values.</li> <li>Examples: Concrete examples that illustrate how to use the code, including input and output.</li> <li>Tips and Variations: Additional information, such as best practices, common pitfalls, and alternative approaches.</li> <li>Related Resources: Links to relevant documentation, tutorials, and external resources.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#best-practices","title":"Best Practices","text":"<p>To ensure high-quality documentation, follow these best practices:</p> <ol> <li>Use headings and subheadings: Organize your content with clear headings and subheadings to facilitate scanning and navigation.</li> <li>Use bullet points and lists: Break up complex information into easy-to-read lists and bullet points.</li> <li>Provide context: Give readers a clear understanding of the code's purpose, history, and relationships to other components.</li> <li>Review and edit: Carefully review and edit your documentation to ensure accuracy, completeness, and consistency.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#resources","title":"Resources","text":"<p>For more information on the Chicago Manual of Style, refer to their online published version.</p> <p>By following these guidelines and principles, you will be able to create high-quality documentation that helps others understand and work with your code effectively. Remember to always prioritize clarity, accuracy, and completeness, and to use the Chicago Style Guide as your reference for writing and formatting.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/","title":"Jupyter Notebook Support","text":"In\u00a0[1]: Copied! <pre>a = 1\nb = 2\na + b\n</pre> a = 1 b = 2 a + b Out[1]: <pre>3</pre> In\u00a0[2]: Copied! <pre>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nxs = np.linspace(0, 2*np.pi, 100)\nplt.plot(xs, np.sin(xs))\n</pre> %matplotlib inline import matplotlib.pyplot as plt import numpy as np  xs = np.linspace(0, 2*np.pi, 100) plt.plot(xs, np.sin(xs)) Out[2]: <pre>[&lt;matplotlib.lines.Line2D at 0x79f258429570&gt;]</pre> In\u00a0[3]: Copied! <pre>#NBVAL_CHECK_OUTPUT\n# pragma: allowlist secret\n\nimport torch\nprint(torch.__version__)\n</pre> #NBVAL_CHECK_OUTPUT # pragma: allowlist secret  import torch print(torch.__version__) <pre>2.3.0a0+ebedce2\n</pre>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#jupyter-notebook-support","title":"Jupyter Notebook Support\u00b6","text":"<p>Jupyter notebooks can be rendered as part of the documentation build system as an alternative to markdown files. The docs site uses mkdocs-jupyter to build and render jupyter notebooks as markdown files.</p> <p>Note: There are some limitations to jupyter rendering.</p> <ol> <li>Notebooks are not executed as part of the docs publishing pipeline. CI tests to ensure notebook consistency are run separately (see Testing Jupyter Notebooks).</li> <li>Notebook markdown cells don't support the full range of mkdocs-material configuration, including things like admonitions, referencing automatically-generated API documentation via mkdocstrings etc. (more here).</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#example-code-block","title":"Example code block\u00b6","text":"<p>Markdown headings can be used to create a TOC similarly to traditional mkdocs pages.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#embedded-visualizations","title":"Embedded visualizations\u00b6","text":"<p>We can also embed images using standard approaches to embedding graphics in notebooks.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#testing-jupyter-notebooks","title":"Testing Jupyter Notebooks\u00b6","text":"<p>Jupyter notebooks are run as part of the CI build suite using <code>nbval</code>. To run these tests locally, run</p> <pre>pytest --nbval-lax docs/\n</pre> <p>from the repository root. By default, <code>nbval</code> will only check that the notebook executes successfully. To add additional checks to ensure the consistency of the output, add a <code>#NBVAL_CHECK_OUTPUT</code> marker comment, which will ensure that the output of the saved jupyter notebook matches the output when the notebook is executed in CI.</p> <p>For example:</p>"},{"location":"user-guide/contributing/Writing%20Documentation/mkdocs/","title":"MkDocs","text":""},{"location":"user-guide/contributing/Writing%20Documentation/mkdocs/#build-system","title":"Build System","text":"<p>This uses Material for MkDocs to build it's documentation. Docstrings are converted to automatically-generated API reference pages using <code>mkdocstrings</code>, and can be linked from markdown pages using paths.</p>"},{"location":"user-guide/developer-guide/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Authenticating Local Docker with NGC</li> <li>Content Metadata</li> <li>NV-Ingest Deployment</li> <li>Environment Configuration Variables</li> <li>Developing with Kubernetes</li> <li>NV-Ingest Command Line (CLI)</li> <li>Telemetry</li> <li>Environment Configuration Variables</li> </ul>"},{"location":"user-guide/developer-guide/content-metadata/","title":"Content Metadata","text":"<p>Definitions:</p> <p>Source: The knowledge base file from which content and metadata is extracted</p> <p>Content: Data extracted from a source: Text or Image</p> <p>Metadata: Descriptive data which can be associated with Sources, Content(Image or Text); metadata can be extracted from Source/Content, or generated using models, heuristics, etc</p> Field Description Method Content Content Content extracted from Source Extracted Source Metadata Source Name Name of source Extracted Source ID ID of source Extracted Source location URL, URI, pointer to storage location N/A Source Type PDF, HTML, Docx, TXT, PPTx Extracted Collection ID Collection in which the source is contained N/A Date Created Date source was created Extracted Last Modified Date source was last modified Extracted Summary Summarization of Source Doc Generated Partition ID Offset of this data fragment within a larger set of fragments Generated Access Level Dictates RBAC N/A Content Metadata (applicable to all content types) Type Text, Image, Structured, Table, Chart Generated Description Text Description of the content object (Image/Table) Generated Page # Page # where content is contained in source Extracted Hierarchy Location/order of content within the source document Extracted Subtype For structured data subtypes - table, chart, etc.. Text Metadata Text Type Header, body, etc Extracted Summary Abbreviated Summary of content Generated Keywords Keywords, Named Entities, or other phrases Extracted Language Generated Image Metadata Image Type Structured, Natural,Hybrid,  etc Generated (Classifier) Structured Image Type Bar Chart, Pie Chart, etc Generated (Classifier) Caption Any caption or subheader associated with Image Extracted Text Extracted text from a structured chart Extracted Image location Location (x,y) of chart within an image Extracted Image location max dimensions Max dimensions (x_max,y_max) of location (x,y) Extracted uploaded_image_uri Mirrors source_metadata.source_location Table Metadata (tables within documents) Table format Structured (dataframe / lists of rows and columns), or serialized as markdown, html, latex, simple (cells separated just as spaces) Extracted Table content Extracted text content, formatted according to table_metadata.table_format. Important: Tables should not be chunked Extracted Table location Bounding box of the table Extracted Table location max dimensions Max dimensions (x_max,y_max) of bounding box of the table Extracted Caption Detected captions for the table/chart Extracted Title TODO Extracted Subtitle TODO Extracted Axis TODO Extracted uploaded_image_uri Mirrors source_metadata.source_location Generated"},{"location":"user-guide/developer-guide/deployment/","title":"NV-Ingest Deployment","text":""},{"location":"user-guide/developer-guide/deployment/#launch-nvidia-microservices","title":"Launch NVIDIA Microservice(s)","text":"<pre><code># Redis is our message broker for the ingest service, always required.\ndocker compose up -d redis\n\n# `yolox`, `deplot`, `cached`, and `paddle` are NIMs used to perform table and chart extraction.\ndocker compose up -d yolox deplot cached paddle\n\n# Optional (MinIO) is an object store to store extracted images, tables, and charts, by default it is commented out in the docker compose file.\n# The `store` task will not be functional without this service or external s3 compliant object store.\ndocker compose up -d minio\n\n# Optional (Milvus) is a vector database to embeddings for multi-model extractions, by default it is commented out in the docker compose file.\n# The `vdb_upload` task will not be functional without this serivce or external Milvus database.\ndocker compose up -d etcd minio milvus attu\n\n# Optional (Telemetry services)\n# TODO: Add examples for telemetry services\ndocker compose up -d otel-collector prometheus grafana zipkin\n\n# Optional (Embedding NIM) Stand up `nv-embedqa-e5-v5` NIM to calculate embeddings for extracted content.\n# The `embed` task will not be functional without this service.\ndocker compose up -d embedding\n\n# Optional (Triton) See below for Triton setup we need Triton for any model inference\n# This is only needed for captioning or DOUGHNUT based extraction.\ndocker compose up -d triton\n\n# Ingest service\ndocker compose up -d nv-ingest-ms-runtime\n</code></pre> <p>You should see something like this:</p> <pre><code>CONTAINER ID   IMAGE                                        COMMAND                 CREATED        STATUS                PORTS                              NAMES\n6065c12d6034   .../nv-ingest:2024.6.3.dev0                 \"/opt/conda/bin/tini\u2026\"   6 hours ago    Up 6 hours                                               nv-ingest-ms-runtime-1\nc1f1f6b9cc8c   .../tritonserver:24.05-py3       \"/opt/nvidia/nvidia_\u2026\"   5 days ago     Up 8 hours            0.0.0.0:8000-8002-&gt;8000-8002/tcp   devin-nv-ingest-triton-1\nd277cf2c2703   redis/redis-stack                           \"/entrypoint.sh\"         2 weeks ago    Up 8 hours            0.0.0.0:6379-&gt;6379/tcp, 8001/tcp   devin-nv-ingest-redis-1\n</code></pre>"},{"location":"user-guide/developer-guide/deployment/#launch-nv-ingest-locally-using-library-api","title":"Launch NV-Ingest Locally Using Library API","text":""},{"location":"user-guide/developer-guide/deployment/#prerequisites","title":"Prerequisites","text":"<p>To run the NV-Ingestt service locally, we require Conda (Mamba) to be installed.</p> <p>From the root of the repository, run the following commands to create a new Conda environment and install the required dependencies:</p> <pre><code>mamba env create --file ./conda/environments/nv_ingest_environment.yml --name nv_ingest_runtime\n\nconda activate nv_ingest_runtime\n\npip install ./\npip install ./client\n</code></pre>"},{"location":"user-guide/developer-guide/environment-config/","title":"Environment Configuration Variables","text":"<p>The following are the environment configuration variables that you can specify in your .env file.</p> Name Example Description <code>CAPTION_CLASSIFIER_GRPC_TRITON</code> - <code>triton:8001</code> The endpoint where the caption classifier model is hosted using gRPC for communication. This is used to send requests for caption classification. You must specify only ONE of an http or gRPC endpoint. If both are specified gRPC will take precedence. <code>CAPTION_CLASSIFIER_MODEL_NAME</code> - <code>deberta_large</code> The name of the caption classifier model. <code>DOUGHNUT_TRITON_HOST</code> - <code>triton-doughnut</code> The hostname or IP address of the DOUGHNUT model service. <code>DOUGHNUT_TRITON_PORT</code> - <code>8001</code> The port number on which the DOUGHNUT model service is listening. <code>INGEST_LOG_LEVEL</code> - <code>DEBUG</code>  - <code>INFO</code>  - <code>WARNING</code>  - <code>ERROR</code>  - <code>CRITICAL</code> The log level for the ingest service, which controls the verbosity of the logging output. <code>MESSAGE_CLIENT_HOST</code> - <code>redis</code>  - <code>localhost</code>  - <code>192.168.1.10</code> Specifies the hostname or IP address of the message broker used for communication between services. <code>MESSAGE_CLIENT_PORT</code> - <code>7670</code>  - <code>6379</code> Specifies the port number on which the message broker is listening. <code>MINIO_BUCKET</code> - <code>nv-ingest</code> Name of MinIO bucket, used to store image, table, and chart extractions. <code>NGC_API_KEY</code> - <code>nvapi-*************</code> An authorized NGC API key, used to interact with hosted NIMs and can be generated here: https://org.ngc.nvidia.com/setup/personal-keys. <code>NIM_NGC_API_KEY</code> \u2014 The key that NIM microservices inside docker containers use to access NGC resources. This is necessary only in some cases when it is different from <code>NGC_API_KEY</code>. If this is not specified, <code>NGC_API_KEY</code> is used to access NGC resources. <code>NVIDIA_BUILD_API_KEY</code> \u2014 The key to access NIMs that are hosted on build.nvidia.com instead of a self-hosted NIM. This is necessary only in some cases when it is different from <code>NGC_API_KEY</code>. If this is not specified, <code>NGC_API_KEY</code> is used for build.nvidia.com. <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> - <code>http://otel-collector:4317</code> The endpoint for the OpenTelemetry exporter, used for sending telemetry data. <code>REDIS_MORPHEUS_TASK_QUEUE</code> - <code>morpheus_task_queue</code> The name of the task queue in Redis where tasks are stored and processed."},{"location":"user-guide/developer-guide/kubernetes-dev/","title":"Developing with Kubernetes","text":"<p>Developing directly on Kubernetes gives us more confidence that end-user deployments will work as expected.</p> <p>This page describes how to use Kubernetes generally and how to deploy nv-ingest on a local Kubernetes cluster.</p> <p>NOTE: Unless otherwise noted, all commands below should be run from the root of this repo.</p>"},{"location":"user-guide/developer-guide/kubernetes-dev/#kubernetes-cluster","title":"Kubernetes Cluster","text":"<p>To start, you need a Kubernetes cluster. We recommend that you use <code>kind</code>, which creates a single Docker container with a Kubernetes cluster inside it.</p> <p>Because the <code>kind</code> cluster needs access to the GPUs on your system, you need to install <code>nvkind</code>. For details, see Running kind clusters with GPUs using nvkind. <code>nvkind</code> provides the following benefits:</p> <ul> <li>Multiple developers on the same system can have isolated Kubernetes clusters</li> <li>Easy to create and delete clusters</li> </ul> <p>From the root of the repo, run the following code to create a configuration file for your cluster.</p> <pre><code>mkdir -p ./.tmp\n\ncat &lt;&lt;EOF &gt; ./.tmp/kind-config.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: nv-ingest-${USER}\nnodes:\n  - role: control-plane\n    image: kindest/node:v1.29.2\n  {{- range $gpu := until numGPUs }}\n  - role: worker\n    extraMounts:\n      # We inject all NVIDIA GPUs using the nvidia-container-runtime.\n      # This requires 'accept-nvidia-visible-devices-as-volume-mounts = true' be set\n      # in '/etc/nvidia-container-runtime/config.toml'\n      - hostPath: /dev/null\n        containerPath: /var/run/nvidia-container-devices/{{ $gpu }}\n  {{- end }}\nEOF\n</code></pre> <p>Then, use the <code>nvkind</code> CLI to create your cluster.</p> <pre><code>nvkind cluster create \\\n    --config-template ./.tmp/kind-config.yaml\n</code></pre> <p>You should see output like this:</p> <pre><code>Creating cluster \"jdyer\" ...\n \u2713 Ensuring node image (kindest/node:v1.27.11) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing CNI \ud83d\udd0c\n \u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-jdyer\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-jdyer\n\nHave a nice day! \ud83d\udc4b\n</code></pre> <p>You can list clusters on the system with <code>kind get clusters</code>.</p> <pre><code>kind get clusters\n# jdyer\n</code></pre> <p>You can also just use <code>docker ps</code> to see the kind container.</p> <pre><code>docker ps | grep kind\n# aaf5216a3cc8   kindest/node:v1.27.11  \"/usr/local/bin/entr\u2026\"   44 seconds ago   Up 42 seconds 127.0.0.1:45099-&gt;6443/tcp jdyer-control-plane\n</code></pre> <p><code>kind create cluster</code> does the following:</p> <ul> <li>Add a context for the cluster to <code>${HOME}/.kube/config</code>, the default config file used by tools like <code>kubectl</code></li> <li>Change the default context to <code>${HOME}/.kube/config</code></li> </ul> <p>You should be able to use <code>kubectl</code> immediately, and it should be pointed at that cluster you just created.</p> <p>For example, try listing notes to verify that the cluster was set up successfully.</p> <pre><code>kubectl get nodes\n</code></pre> <p>If that worked, you should see a single node like this:</p> <pre><code>NAME                  STATUS   ROLES           AGE   VERSION\njdyer-control-plane   Ready    control-plane   63s   v1.27.11\n</code></pre> <p>Note: Not all of the containers created inside your Kubernetes cluster appear when you run <code>docker ps</code> because some of them are are nested within a separate namespace.</p> <p>For help with issues that arise, see Troubleshooting.</p>"},{"location":"user-guide/developer-guide/kubernetes-dev/#skaffold","title":"Skaffold","text":"<p>Now that you have a Kubernetes cluster, you can use Skaffold to build and deploy your development environment.</p> <p>In a single command, Skaffold does the following:</p> <ul> <li>Build containers from the current directory (via <code>docker build</code>)</li> <li>Install the retriever-ingest helm charts (via <code>helm install</code>)</li> <li>Apply additional Kubernetes manifests (via <code>kustomize</code>)</li> <li>Hot reloading - Skaffold watches your local directory for changes and syncs them into the Kubernetes container</li> <li>Port forwards the ingest service to the host</li> </ul>"},{"location":"user-guide/developer-guide/kubernetes-dev/#directory-structure","title":"Directory Structure","text":"<ul> <li><code>skaffold/sensitive/</code> contains any secrets or manifests you want deployed to your cluster but not checked into git, as your local cluster is unlikely to have ESO installed. If it does, feel free to use <code>kind: ExternalSecret</code> instead.</li> <li><code>skaffold/components</code> contains any k8s manifests you want deployed in any skaffold file. The paths are relative and can be used in either <code>kustomize</code> or <code>rawYaml</code> formats:</li> </ul> <pre><code>manifests:\n  rawYaml:\n    - sensitive/*.yaml\n  kustomize:\n    paths:\n      - components/elasticsearch\n</code></pre> <ul> <li>If adding a new service, try getting a helm object first. If none exists, you may have to encapsulate it with your k8s manifests in <code>skaffold/components</code>. We are a k8s shop, so manifest writing may be required from time to time.</li> </ul>"},{"location":"user-guide/developer-guide/kubernetes-dev/#prerequisites","title":"Prerequisites","text":""},{"location":"user-guide/developer-guide/kubernetes-dev/#add-helm-repos","title":"Add Helm Repos","text":"<p>The retriever-ingest service's deployment requires pulling in configurations for other services from third-party sources, such as Elasticsearch, OpenTelemetry, and Postgres.</p> <p>The first time you deploy this project to a local Kubernetes, you might need to tell your local version of <code>Helm</code> (a package manager for Kubernetes configurations) where to find third-party services by running code similar to the following.</p> <pre><code>helm repo add \\\n  nvdp \\\n  https://nvidia.github.io/k8s-device-plugin\n\nhelm repo add \\\n  zipkin \\\n  https://zipkin.io/zipkin-helm\n\nhelm repo add \\\n  opentelemetry \\\n  https://open-telemetry.github.io/opentelemetry-helm-charts\n\nhelm repo add \\\n  nvidia \\\n  https://helm.ngc.nvidia.com/nvidia\n\nhelm repo add \\\n  bitnami \\\n  https://charts.bitnami.com/bitnami\n</code></pre> <p>For the full list of repositories, refer to the <code>dependencies</code> section in the Chart.yaml file.</p>"},{"location":"user-guide/developer-guide/kubernetes-dev/#nvidia-gpu-support","title":"NVIDIA GPU Support","text":"<p>For the Kubernetes pods to access the NVIDIA GPU resources, you must install the NVIDIA device plugin for Kubernetes. There are many configurations for this plugin, but to start development simply run the following code.</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.15.0/deployments/static/nvidia-device-plugin.yml\n</code></pre>"},{"location":"user-guide/developer-guide/kubernetes-dev/#create-an-image-pull-secret","title":"Create an Image Pull Secret","text":"<p>You'll also need to provide a Kubernetes Secret with credentials to pull NVIDIA-private Docker images.</p> <p>For short-lived development clusters, just use your own individual credentials.</p> <pre><code>DOCKER_CONFIG_JSON=$(\n    cat \"${HOME}/.docker/config.json\" \\\n    | base64 -w 0\n)\n\ncat &lt;&lt;EOF &gt; ./skaffold/sensitive/imagepull.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: nvcrimagepullsecret\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: ${DOCKER_CONFIG_JSON}\nEOF\n</code></pre> <p>You need an NGC personal API key to access models and images that are hosted on NGC. First, Generate an API key. Next, store the key in an environment variable by running the following code.</p> <pre><code>export NGC_API_KEY=\"&lt;YOUR_KEY_HERE&gt;\"\n</code></pre> <p>Then, create the secret manifest with:</p> <pre><code>kubectl create secret generic ngcapisecrets \\\n  --from-literal=ngc_api_key=\"${NGC_API_KEY}\" \\\n  --dry-run=client -o yaml \\\n  &gt; skaffold/sensitive/ngcapi.yaml\n</code></pre>"},{"location":"user-guide/developer-guide/kubernetes-dev/#deploy-the-service","title":"Deploy the Service","text":"<p>Run the following to deploy the retriever-ingest to your cluster.</p> <pre><code>skaffold dev \\\n    -v info \\\n    -f ./skaffold/nv-ingest.skaffold.yaml \\\n    --kube-context \"kind-nv-ingest-${USER}\"\n</code></pre> explanation of those flags (click me)  - `-v info` = print INFO-level and above logs from `skaffold` and the tools it calls (like `helm` or `kustomize`) - `-f ./skaffold/nv-ingest.skaffold.yaml` = use configuration specific to retriever-ingest - `--tail=false` = don't flood your console with all the logs from the deployed containers - `--kube-context \"kind-${USER}\"` = target the specific Kubernetes cluster you created with `kind` above   <p><code>skaffold dev</code> watches your local files and automatically redeploys the app as you change those files. It also holds control in the terminal you run it in, and handles shutting down the pods in Kubernetes when you <code>Ctrl + C</code> out of it.</p> <p>You should see output similar to this:</p> <pre><code>Generating tags...\n - ...\nChecking cache...\n - ...\nTags used in deployment:\n - ...\nStarting deploy...\nLoading images into kind cluster nodes...\n - ...\nWaiting for deployments to stabilize...\nDeployments stabilized in 23.08 seconds\nWatching for changes...\n</code></pre> <p>When you run this command, <code>skaffold dev</code> finds a random open port on the system and exposes the retriever-ingest service on that port. For more information, see Port Forwarding.</p> <p>You can find that port in <code>skaffold</code>'s logs by running the following code.</p> <pre><code>Port forwarding Service/nv-ingest in namespace , remote port http -&gt; http://0.0.0.0:4503\n</code></pre> <p>Alternatively, you can obtain it like this:</p> <pre><code>NV_INGEST_MS_PORT=$(\n    ps aux \\\n    | grep -E \"kind\\-${USER} port-forward .*Service/nv-ingest\" \\\n    | grep -o -E '[0-9]+:http' \\\n    | cut -d ':' -f1\n)\n</code></pre> <p>To confirm that the service is deployed and working, issue a request against the port you set up port-forwarding to above.</p> <pre><code>API_HOST=\"http://localhost:${NV_INGEST_MS_PORT}\"\n\ncurl \\\n  -i \\\n  -X GET \\\n  \"${API_HOST}/health\"\n</code></pre> <p>When you run <code>skaffold verify</code> in a new terminal, Skaffold runs verification tests against the service. These are very lightweight health checks, and should not be confused with integration tests. For more information, see Verify.</p>"},{"location":"user-guide/developer-guide/kubernetes-dev/#clean-up","title":"Clean Up","text":"<p>To destroy the entire Kubernetes cluster, run the following.</p> <pre><code>kind delete cluster \\\n    --name \"${USER}\"\n</code></pre>"},{"location":"user-guide/developer-guide/kubernetes-dev/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/developer-guide/kubernetes-dev/#debugging-tools","title":"Debugging Tools","text":"<p><code>kubectl</code> is the official CLI for Kubernetes and supports a lot of useful functionality.</p> <p>For example, to get a shell inside the <code>nv-ingest-ms-runtime</code> container in your deployment, run the following:</p> <pre><code>NV_INGEST_POD=$(\n    kubectl get pods \\\n        --context \"kind-${USER}\" \\\n        --namespace default \\\n        -l 'app.kubernetes.io/instance=nv-ingest-ms-runtime' \\\n        --no-headers \\\n    | awk '{print $1}'\n)\nkubectl exec \\\n    --context \"kind-${USER}\" \\\n    --namespace default \\\n    pod/${NV_INGEST_POD} \\\n    -i \\\n    -t \\\n    -- sh\n</code></pre> <p>For an interactive, live-updating experience, try k9s. To launch it, run <code>k9s</code>.</p> <pre><code>k9s\n</code></pre>"},{"location":"user-guide/developer-guide/kubernetes-dev/#installing-helm-repositories","title":"Installing Helm Repositories","text":"<p>You could encounter an error like the following. This indicates that your local installation of <code>Helm</code> (a package manager for Kubernetes configurations) doesn't know how to access a remote repository containing Kubernetes configurations.</p> <pre><code>Error: no repository definition for https://helm.dask.org. Please add the missing repos via 'helm repo add'\n</code></pre> <p>To resolve this issue, run <code>help repo add</code> with the URL and an informative name.</p> <pre><code>helm repo add \\\n    bitnami \\\n    https://charts.bitnami.com/bitnami\n</code></pre>"},{"location":"user-guide/developer-guide/kubernetes-dev/#getting-more-logs-from-skaffold","title":"Getting More Logs From <code>skaffold</code>","text":"<p>You could encounter an error like this:</p> <pre><code>Generating tags...\n - retrieval-ms -&gt; retrieval-ms:f181a78-dirty\nChecking cache...\n - retrieval-ms: Found Locally\nCleaning up...\n - No resources found\nbuilding helm dependencies: exit status 1\n</code></pre> <p>If you only see <code>building helm dependencies</code>, you probably ran <code>skaffold dev</code> or <code>skaffold run</code> in quiet mode. Rerun the commands with <code>-v info</code> or <code>-v debug</code> to get more information about what failed.</p>"},{"location":"user-guide/developer-guide/kubernetes-dev/#references","title":"References","text":"<ul> <li>Helm Quickstart</li> <li>Kind Quickstart</li> <li>Skaffold Quickstart</li> </ul>"},{"location":"user-guide/developer-guide/ngc-api-key/","title":"Authenticating Local Docker with NGC","text":""},{"location":"user-guide/developer-guide/ngc-api-key/#generate-an-api-key","title":"Generate an API key","text":"<p>NGC contains many public images, models, and datasets that can be pulled immediately without authentication. To push and pull custom images to and from the private registry, you must authenticate with NGC and generate a private key.</p> <p>To create a private key, go to https://org.ngc.nvidia.com/setup/personal-keys.</p> <p>When creating an NGC API key, ensure that all of the following are selected from the \"Services Included\" drop-down. - AI Foundation Models and Endpoints - NGC Catalog - Private Registry</p> <p></p>"},{"location":"user-guide/developer-guide/ngc-api-key/#docker-login-to-ngc","title":"Docker Login to NGC","text":"<p>To pull the NIM container image from NGC, use your API key to log in to the NGC registry by entering the following command and following the prompts: <pre><code>$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre> For the username, enter '$oauthtoken' exactly as shown. It is a special authentication key for all users.</p>"},{"location":"user-guide/developer-guide/nv-ingest_cli/","title":"NV-Ingest Command Line (CLI)","text":"<p>After installing the Python dependencies, you'll be able to use the nv-ingest-cli tool.</p> <pre><code>nv-ingest-cli --help\nUsage: nv-ingest-cli [OPTIONS]\n\nOptions:\n  --batch_size INTEGER            Batch size (must be &gt;= 1).  [default: 10]\n  --doc PATH                      Add a new document to be processed (supports\n                                  multiple).\n  --dataset PATH                  Path to a dataset definition file.\n  --client [REST|REDIS|KAFKA]     Client type.  [default: REDIS]\n  --client_host TEXT              DNS name or URL for the endpoint.\n  --client_port INTEGER           Port for the client endpoint.\n  --client_kwargs TEXT            Additional arguments to pass to the client.\n  --concurrency_n INTEGER         Number of inflight jobs to maintain at one\n                                  time.  [default: 10]\n  --document_processing_timeout INTEGER\n                                  Timeout when waiting for a document to be\n                                  processed.  [default: 10]\n  --dry_run                       Perform a dry run without executing actions.\n  --output_directory PATH         Output directory for results.\n  --log_level [DEBUG|INFO|WARNING|ERROR|CRITICAL]\n                                  Log level.  [default: INFO]\n  --shuffle_dataset               Shuffle the dataset before processing.\n                                  [default: True]\n  --task TEXT                     Task definitions in JSON format, allowing multiple tasks to be configured by repeating this option.\n                                  Each task must be specified with its type and corresponding options in the '[task_id]:{json_options}' format.\n\n                                  Example:\n                                    --task 'split:{\"split_by\":\"page\", \"split_length\":10}'\n                                    --task 'extract:{\"document_type\":\"pdf\", \"extract_text\":true}'\n                                    --task 'extract:{\"document_type\":\"pdf\", \"extract_method\":\"doughnut\"}'\n                                    --task 'extract:{\"document_type\":\"pdf\", \"extract_method\":\"unstructured_io\"}'\n                                    --task 'extract:{\"document_type\":\"docx\", \"extract_text\":true, \"extract_images\":true}'\n                                    --task 'store:{\"content_type\":\"image\", \"store_method\":\"minio\", \"endpoint\":\"minio:9000\"}'\n                                    --task 'store:{\"content_type\":\"image\", \"store_method\":\"minio\", \"endpoint\":\"minio:9000\", \"text_depth\": \"page\"}'\n                                    --task 'caption:{}'\n\n                                  Tasks and Options:\n                                  - split: Divides documents according to specified criteria.\n                                      Options:\n                                      - split_by (str): Criteria ('page', 'size', 'word', 'sentence'). No default.\n                                      - split_length (int): Segment length. No default.\n                                      - split_overlap (int): Segment overlap. No default.\n                                      - max_character_length (int): Maximum segment character count. No default.\n                                      - sentence_window_size (int): Sentence window size. No default.\n\n                                  - extract: Extracts content from documents, customizable per document type.\n                                      Can be specified multiple times for different 'document_type' values.\n                                      Options:\n                                      - document_type (str): Document format ('pdf', 'docx', 'pptx', 'html', 'xml', 'excel', 'csv', 'parquet'). Required.\n                                      - text_depth (str): Depth at which text parsing occurs ('document', 'page'), additional text_depths are partially supported and depend on the specified extraction method ('block', 'line', 'span')\n                                      - extract_method (str): Extraction technique. Defaults are smartly chosen based on 'document_type'.\n                                      - extract_text (bool): Enables text extraction. Default: False.\n                                      - extract_images (bool): Enables image extraction. Default: False.\n                                      - extract_tables (bool): Enables table extraction. Default: False.\n\n                                  - store: Stores any images extracted from documents.\n                                      Options:\n                                      - structured (bool):  Flag to write extracted charts and tables to object store. Default: True.\n                                      - images (bool): Flag to write extracted images to object store. Default: False.\n                                      - store_method (str): Storage type ('minio', ). Required.\n\n                                  - caption: Attempts to extract captions for images extracted from documents. Note: this is not generative, but rather a\n                                      simple extraction.\n                                      Options:\n                                        N/A\n\n                                  - dedup: Idenfities and optionally filters duplicate images in extraction.\n                                      Options:\n                                        - content_type (str): Content type to deduplicate ('image')\n                                        - filter (bool): When set to True, duplicates will be filtered, otherwise, an info message will be added.\n\n                                  - filter: Idenfities and optionally filters images above or below scale thresholds.\n                                      Options:\n                                        - content_type (str): Content type to deduplicate ('image')\n                                        - min_size: (Union[float, int]): Minimum allowable size of extracted image.\n                                        - max_aspect_ratio: (Union[float, int]): Maximum allowable aspect ratio of extracted image.\n                                        - min_aspect_ratio: (Union[float, int]): Minimum allowable aspect ratio of extracted image.\n                                        - filter (bool): When set to True, duplicates will be filtered, otherwise, an info message will be added.\n\n                                  Note: The 'extract_method' automatically selects the optimal method based on 'document_type' if not explicitly stated.\n  --version                       Show version.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"user-guide/developer-guide/nv-ingest_cli/#example-document-submission-to-the-nv-ingest-ms-runtime-service","title":"Example document submission to the nv-ingest-ms-runtime service","text":"<p>Each of the following can be run from the host machine or from within the nv-ingest-ms-runtime container.</p> <ul> <li>Host: <code>nv-ingest-cli ...</code></li> <li>Container: <code>nv-ingest-cli ...</code></li> </ul> <p>Submit a text file, with no splitting.</p> <p>Note: You will receive a response containing a single document, which is the entire text file -- This is mostly a NO-OP, but the returned data will be wrapped in the appropriate metadata structure.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>Submit a PDF file with only a splitting task.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>Submit a PDF file with splitting and extraction tasks.</p> <p>Note: (TODO) This currently only works for pdfium, doughnut, and Unstructured.io; haystack, Adobe, and LlamaParse have existing workflows but have not been fully converted to use our unified metadata schema.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --task='extract:{\"document_type\": \"docx\", \"extract_method\": \"python_docx\"}' \\\n  --task='split' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>Submit a dataset for processing</p> <pre><code>nv-ingest-cli \\\n  --dataset dataset.json \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>Submit a PDF file with extraction tasks and upload extracted images to MinIO.</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\"}' \\\n  --task='store:{\"endpoint\":\"minio:9000\",\"access_key\":\"minioadmin\",\"secret_key\":\"minioadmin\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre>"},{"location":"user-guide/developer-guide/nv-ingest_cli/#command-line-dataset-creation-with-enumeration-and-sampling","title":"Command Line Dataset Creation with Enumeration and Sampling","text":""},{"location":"user-guide/developer-guide/nv-ingest_cli/#gen_datasetpy","title":"gen_dataset.py","text":"<pre><code>python ./src/util/gen_dataset.py --source_directory=./data --size=1GB --sample pdf=60 --sample txt=40 --output_file \\\n  dataset.json --validate-output\n</code></pre> <p>This script samples files from a specified source directory according to defined proportions and a total size target. It offers options for caching the file list, outputting a sampled file list, and validating the output.</p>"},{"location":"user-guide/developer-guide/nv-ingest_cli/#options","title":"Options","text":"<ul> <li> <p><code>--source_directory</code>: Specifies the path to the source directory where files will be scanned for sampling.</p> </li> <li> <p>Type: String</p> </li> <li>Required: Yes</li> <li> <p>Example: <code>--source_directory ./data</code></p> </li> <li> <p><code>--size</code>: Defines the total size of files to sample. You can use suffixes (KB, MB, GB).</p> </li> <li> <p>Type: String</p> </li> <li>Required: Yes</li> <li> <p>Example: <code>--size 500MB</code></p> </li> <li> <p><code>--sample</code>: Specifies file types and their proportions of the total size. Can be used multiple times for different   file types.</p> </li> <li> <p>Type: String</p> </li> <li>Required: No</li> <li>Multiple: Yes</li> <li> <p>Example: <code>--sample pdf=40 --sample txt=60</code></p> </li> <li> <p><code>--cache_file</code>: If provided, caches the scanned file list as JSON at this path.</p> </li> <li> <p>Type: String</p> </li> <li>Required: No</li> <li> <p>Example: <code>--cache_file ./file_list_cache.json</code></p> </li> <li> <p><code>--output_file</code>: If provided, outputs the list of sampled files as JSON at this path.</p> </li> <li> <p>Type: String</p> </li> <li>Required: No</li> <li> <p>Example: <code>--output_file ./sampled_files.json</code></p> </li> <li> <p><code>--validate-output</code>: If set, the script re-validates the <code>output_file</code> JSON and logs total bytes for each file type.</p> </li> <li> <p>Type: Flag</p> </li> <li> <p>Required: No</p> </li> <li> <p><code>--log-level</code>: Sets the logging level ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'). Default is 'INFO'.</p> </li> <li> <p>Type: Choice</p> </li> <li>Required: No</li> <li> <p>Example: <code>--log-level DEBUG</code></p> </li> <li> <p><code>--with-replacement</code>: Sample with replacement. Files can be selected multiple times.</p> </li> <li>Type: Flag</li> <li>Default: True (if omitted, sampling will be with replacement)</li> <li>Usage Example: <code>--with-replacement</code> to enable sampling with replacement or omit for default behavior.     Use <code>--no-with-replacement</code> to disable it and sample without replacement.</li> </ul> <p>The script performs a sampling process that respects the specified size and type proportions, generates a detailed file list, and provides options for caching and validation to facilitate efficient data handling and integrity checking.</p> <p>Command line interface for the Image Viewer application, displays paginated images from a JSON file viewer. Each image is resized for uniform display, and users can navigate through the images using \"Next\" and \"Previous\" buttons.</p>"},{"location":"user-guide/developer-guide/nv-ingest_cli/#image_viewerpy","title":"image_viewer.py","text":"<ul> <li><code>--file_path</code>: Specifies the path to the JSON file containing the images. The JSON file should contain a list of   objects, each with an <code>\"image\"</code> field that includes a base64 encoded string of the image data.</li> <li>Type: String</li> <li>Required: Yes</li> <li>Example Usage:     <pre><code>--file_path \"/path/to/your/images.json\"\n</code></pre></li> </ul>"},{"location":"user-guide/developer-guide/telemetry/","title":"Telemetry","text":""},{"location":"user-guide/developer-guide/telemetry/#docker-compose","title":"Docker Compose","text":"<p>To run OpenTelemetry locally, run:</p> <pre><code>$ docker compose up otel-collector\n</code></pre> <p>Once OpenTelemetry and Zipkin are running, you can open your browser to explore traces: http://$YOUR_DOCKER_HOST:9411/zipkin/.</p> <p></p> <p>To run Prometheus, run:</p> <pre><code>$ docker compose up prometheus\n</code></pre> <p>Once Promethus is running, you can open your browser to explore metrics: [http://$YOUR_DOCKER_HOST:9090/]</p> <p></p>"},{"location":"user-guide/getting-started/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Hardware and Software Prerequisites</li> <li>Quickstart Guide</li> </ul>"},{"location":"user-guide/getting-started/prerequisites/","title":"Prerequisites","text":"<p>Before you begin using NVIDIA-Ingest, ensure the following hardware and software prerequisites outlined are met.</p>"},{"location":"user-guide/getting-started/prerequisites/#hardware","title":"Hardware","text":"GPU Family Memory # of GPUs (min.) H100 SXM or PCIe 80GB 2 A100 SXM or PCIe 80GB 2"},{"location":"user-guide/getting-started/prerequisites/#software","title":"Software","text":"<ul> <li>Linux operating systems (Ubuntu 22.04 or later recommended)</li> <li>Docker</li> <li>Docker Compose</li> <li>CUDA Toolkit (NVIDIA Driver &gt;= <code>535</code>, CUDA &gt;= <code>12.2</code>)</li> <li>NVIDIA Container Toolkit</li> </ul> <p>[!Note] You install Python later. NVIDIA-Ingest only supports Python version 3.10.</p>"},{"location":"user-guide/getting-started/quickstart-guide/","title":"Quickstart Guide","text":"<p>To get started using NVIDIA-Ingest, you need to do a few things:</p> <ol> <li>Start supporting NIM microservices \ud83c\udfd7\ufe0f</li> <li>Install the NVIDIA Ingest client dependencies in a Python environment \ud83d\udc0d</li> <li>Submit ingestion job(s) \ud83d\udcd3</li> <li>Inspect and consume results \ud83d\udd0d</li> </ol>"},{"location":"user-guide/getting-started/quickstart-guide/#step-1-starting-containers","title":"Step 1: Starting Containers","text":"<p>This example demonstrates how to use the provided docker-compose.yaml to start all needed services with a few commands.</p> <p>IMPORTANT: NIM containers on their first startup can take 10-15 minutes to pull and fully load models.</p> <p>If preferred, you can also start services one by one or run on Kubernetes via our Helm chart. Also, there are additional environment variables you want to configure.</p> <ol> <li>Git clone the repo: <code>git clone https://github.com/nvidia/nv-ingest</code></li> <li> <p>Change the directory to the cloned repo <code>cd nv-ingest</code>.</p> </li> <li> <p>Generate API keys and authenticate with NGC with the <code>docker login</code> command: <pre><code># This is required to access pre-built containers and NIM microservices\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: &lt;Your Key&gt;\n</code></pre></p> </li> </ol> <p>[!NOTE] During the early access (EA) phase, you must apply for early access here: https://developer.nvidia.com/nemo-microservices-early-access/join.  When your early access is approved, follow the instructions in the email to create an organization and team, link your profile, and generate your NGC API key.</p> <ol> <li>Create a .env file containing your NGC API key and the following paths. For more information, refer to Environment Configuration Variables.</li> </ol> <pre><code># Container images must access resources from NGC.\n\nNGC_API_KEY=&lt;key to download containers from NGC&gt;\nNIM_NGC_API_KEY=&lt;key to download model files after containers start&gt;\nNVIDIA_BUILD_API_KEY=&lt;key to use NIMs that are hosted on build.nvidia.com&gt;\n\nDATASET_ROOT=&lt;PATH_TO_THIS_REPO&gt;/data\nNV_INGEST_ROOT=&lt;PATH_TO_THIS_REPO&gt;\n</code></pre> <p>NOTE: As configured by default in docker-compose.yaml, the DePlot NIM is on a dedicated GPU. All other NIMs and the NV-Ingest container itself share a second. This avoids DePlot and other NIMs competing for VRAM on the same device.</p> <p>Change the <code>CUDA_VISIBLE_DEVICES</code> pinnings as desired for your system within docker-compose.yaml.</p> <p>IMPORTANT: Make sure NVIDIA is set as your default container runtime before running the docker compose command with the command:</p> <p><code>sudo nvidia-ctk runtime configure --runtime=docker --set-as-default</code></p> <ol> <li>Start all services: <code>docker compose up</code></li> </ol> <p>TIP: By default, we have [configured log levels to be verbose](docker-compose.yaml.</p> <p>It's possible to observe service startup proceeding. You will notice a lot of log messages. Disable verbose logging by configuring <code>NIM_TRITON_LOG_VERBOSE=0</code> for each NIM in docker-compose.yaml.</p> <ol> <li>When all services have fully started, <code>nvidia-smi</code> should show processes like the following: <pre><code># If it's taking &gt; 1m for `nvidia-smi` to return, the bus will likely be busy setting up the models.\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A   1352957      C   tritonserver                                762MiB |\n|    1   N/A  N/A   1322081      C   /opt/nim/llm/.venv/bin/python3            63916MiB |\n|    2   N/A  N/A   1355175      C   tritonserver                                478MiB |\n|    2   N/A  N/A   1367569      C   ...s/python/triton_python_backend_stub       12MiB |\n|    3   N/A  N/A   1321841      C   python                                      414MiB |\n|    3   N/A  N/A   1352331      C   tritonserver                                478MiB |\n|    3   N/A  N/A   1355929      C   ...s/python/triton_python_backend_stub      424MiB |\n|    3   N/A  N/A   1373202      C   tritonserver                                414MiB |\n+---------------------------------------------------------------------------------------+\n</code></pre></li> </ol> <p>Observe the started containers with <code>docker ps</code>: <pre><code>CONTAINER ID   IMAGE                                                                      COMMAND                  CREATED          STATUS                    PORTS                                                                                                                                                                                                                                                                                NAMES\n0f2f86615ea5   nvcr.io/nvidia/nemo-microservices/nv-ingest:24.12                       \"/opt/conda/bin/tini\u2026\"   35 seconds ago   Up 33 seconds             0.0.0.0:7670-&gt;7670/tcp, :::7670-&gt;7670/tcp                                                                                                                                                                                                                                            nv-ingest-nv-ingest-ms-runtime-1\nde44122c6ddc   otel/opentelemetry-collector-contrib:0.91.0                                \"/otelcol-contrib --\u2026\"   14 hours ago     Up 24 seconds             0.0.0.0:4317-4318-&gt;4317-4318/tcp, :::4317-4318-&gt;4317-4318/tcp, 0.0.0.0:8888-8889-&gt;8888-8889/tcp, :::8888-8889-&gt;8888-8889/tcp, 0.0.0.0:13133-&gt;13133/tcp, :::13133-&gt;13133/tcp, 55678/tcp, 0.0.0.0:32849-&gt;9411/tcp, :::32848-&gt;9411/tcp, 0.0.0.0:55680-&gt;55679/tcp, :::55680-&gt;55679/tcp   nv-ingest-otel-collector-1\n02c9ab8c6901   nvcr.io/nvidia/nemo-microservices/cached:0.2.0                          \"/opt/nvidia/nvidia_\u2026\"   14 hours ago     Up 24 seconds             0.0.0.0:8006-&gt;8000/tcp, :::8006-&gt;8000/tcp, 0.0.0.0:8007-&gt;8001/tcp, :::8007-&gt;8001/tcp, 0.0.0.0:8008-&gt;8002/tcp, :::8008-&gt;8002/tcp                                                                                                                                                      nv-ingest-cached-1\nd49369334398   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.1.0                                  \"/opt/nvidia/nvidia_\u2026\"   14 hours ago     Up 33 seconds             0.0.0.0:8012-&gt;8000/tcp, :::8012-&gt;8000/tcp, 0.0.0.0:8013-&gt;8001/tcp, :::8013-&gt;8001/tcp, 0.0.0.0:8014-&gt;8002/tcp, :::8014-&gt;8002/tcp                                                                                                                                                      nv-ingest-embedding-1\n508715a24998   nvcr.io/nvidia/nemo-microservices/nv-yolox-structured-images-v1:0.2.0   \"/opt/nvidia/nvidia_\u2026\"   14 hours ago     Up 33 seconds             0.0.0.0:8000-8002-&gt;8000-8002/tcp, :::8000-8002-&gt;8000-8002/tcp                                                                                                                                                                                                                        nv-ingest-yolox-1\n5b7a174a0a85   nvcr.io/nvidia/nemo-microservices/deplot:1.0.0                          \"/opt/nvidia/nvidia_\u2026\"   14 hours ago     Up 33 seconds             0.0.0.0:8003-&gt;8000/tcp, :::8003-&gt;8000/tcp, 0.0.0.0:8004-&gt;8001/tcp, :::8004-&gt;8001/tcp, 0.0.0.0:8005-&gt;8002/tcp, :::8005-&gt;8002/tcp                                                                                                                                                      nv-ingest-deplot-1\n430045f98c02   nvcr.io/nvidia/nemo-microservices/paddleocr:0.2.0                       \"/opt/nvidia/nvidia_\u2026\"   14 hours ago     Up 24 seconds             0.0.0.0:8009-&gt;8000/tcp, :::8009-&gt;8000/tcp, 0.0.0.0:8010-&gt;8001/tcp, :::8010-&gt;8001/tcp, 0.0.0.0:8011-&gt;8002/tcp, :::8011-&gt;8002/tcp                                                                                                                                                      nv-ingest-paddle-1\n8e587b45821b   grafana/grafana                                                            \"/run.sh\"                14 hours ago     Up 33 seconds             0.0.0.0:3000-&gt;3000/tcp, :::3000-&gt;3000/tcp                                                                                                                                                                                                                                            grafana-service\naa2c0ec387e2   redis/redis-stack                                                          \"/entrypoint.sh\"         14 hours ago     Up 33 seconds             0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp, 8001/tcp                                                                                                                                                                                                                                  nv-ingest-redis-1\nbda9a2a9c8b5   openzipkin/zipkin                                                          \"start-zipkin\"           14 hours ago     Up 33 seconds (healthy)   9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp                                                                                                                                                                                                                                  nv-ingest-zipkin-1\nac27e5297d57   prom/prometheus:latest                                                     \"/bin/prometheus --w\u2026\"   14 hours ago     Up 33 seconds             0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp                                                                                                                                                                                                                                            nv-ingest-prometheus-1\n</code></pre></p> <p>TIP: NV-Ingest is in early access (EA) mode, meaning the codebase gets frequent updates. To build an updated NV-Ingest service container with the latest changes, you can: <pre><code>docker compose build\n</code></pre> After the image is built, run <code>docker compose up</code> per item 5 above.</p>"},{"location":"user-guide/getting-started/quickstart-guide/#step-2-installing-python-dependencies","title":"Step 2: Installing Python Dependencies","text":"<p>You can interact with the NV-Ingest service from the host or by <code>docker exec</code>-ing into the NV-Ingest container.</p> <p>To interact from the host, you'll need a Python environment and install the client dependencies: <pre><code># conda not required but makes it easy to create a fresh Python environment\nconda create --name nv-ingest-dev python=3.10\nconda activate nv-ingest-dev\ncd client\npip install .\n</code></pre></p> <p>NOTE: Interacting from the host depends on the appropriate port being exposed from the nv-ingest container to the host as defined in docker-compose.yaml.</p> <p>If you prefer, you can disable exposing that port and interact with the NV-Ingest service directly from within its container.</p> <p>To interact within the container: <pre><code>docker exec -it nv-ingest-nv-ingest-ms-runtime-1 bash\n</code></pre> You'll be in the <code>/workspace</code> directory with <code>DATASET_ROOT</code> from the .env file mounted at <code>./data</code>. The pre-activated <code>morpheus</code> conda environment has all the Python client libraries pre-installed: <pre><code>(morpheus) root@aba77e2a4bde:/workspace#\n</code></pre></p> <p>From the bash prompt above, you can run the nv-ingest-cli and Python examples described below.</p>"},{"location":"user-guide/getting-started/quickstart-guide/#step-3-ingesting-documents","title":"Step 3: Ingesting Documents","text":"<p>You can submit jobs programmatically in Python or using the nv-ingest-cli tool.</p> <p>In the below examples, we are doing text, chart, table, and image extraction: - <code>extract_text</code>, - uses PDFium to find and extract text from pages - <code>extract_images</code> - uses PDFium to extract images - <code>extract_tables</code> - uses YOLOX to find tables and charts. Uses PaddleOCR for table extraction, and Deplot and CACHED for chart extraction - <code>extract_charts</code> - (optional) enables or disables the use of Deplot and CACHED for chart extraction.</p> <p>IMPORTANT: <code>extract_tables</code> controls extraction for both tables and charts. You can optionally disable chart extraction by setting <code>extract_charts</code> to false.</p>"},{"location":"user-guide/getting-started/quickstart-guide/#in-python","title":"In Python","text":"<p>You can find more documentation and examples here:</p> <pre><code>import logging, time\n\nfrom nv_ingest_client.client import NvIngestClient\nfrom nv_ingest_client.primitives import JobSpec\nfrom nv_ingest_client.primitives.tasks import ExtractTask\nfrom nv_ingest_client.util.file_processing.extract import extract_file_content\n\nlogger = logging.getLogger(\"nv_ingest_client\")\n\nfile_name = \"data/multimodal_test.pdf\"\nfile_content, file_type = extract_file_content(file_name)\n\n# A JobSpec is an object that defines a document and how it should\n# be processed by the nv-ingest service.\njob_spec = JobSpec(\n  document_type=file_type,\n  payload=file_content,\n  source_id=file_name,\n  source_name=file_name,\n  extended_options=\n    {\n      \"tracing_options\":\n      {\n        \"trace\": True,\n        \"ts_send\": time.time_ns()\n      }\n    }\n)\n\n# configure desired extraction modes here. Multiple extraction\n# methods can be defined for a single JobSpec\nextract_task = ExtractTask(\n  document_type=file_type,\n  extract_text=True,\n  extract_images=True,\n  extract_tables=True\n)\n\njob_spec.add_task(extract_task)\n\n# Create the client and inform it about the JobSpec we want to process.\nclient = NvIngestClient(\n  message_client_hostname=\"localhost\", # Host where nv-ingest-ms-runtime is running\n  message_client_port=7670 # REST port, defaults to 7670\n)\njob_id = client.add_job(job_spec)\nclient.submit_job(job_id, \"morpheus_task_queue\")\nresult = client.fetch_job_result(job_id, timeout=60)\nprint(f\"Got {len(result)} results\")\n</code></pre>"},{"location":"user-guide/getting-started/quickstart-guide/#using-the-nv-ingest-cli","title":"Using the <code>nv-ingest-cli</code>","text":"<p>You can find more nv-ingest-cli examples here:</p> <pre><code>nv-ingest-cli \\\n  --doc ./data/multimodal_test.pdf \\\n  --output_directory ./processed_docs \\\n  --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_tables\": \"true\", \"extract_images\": \"true\"}' \\\n  --client_host=localhost \\\n  --client_port=7670\n</code></pre> <p>You should notice output indicating document processing status followed by a breakdown of time spent during job execution: <pre><code>INFO:nv_ingest_client.nv_ingest_cli:Processing 1 documents.\nINFO:nv_ingest_client.nv_ingest_cli:Output will be written to: ./processed_docs\nProcessing files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10&lt;00:00, 10.47s/file, pages_per_sec=0.29]\nINFO:nv_ingest_client.cli.util.processing:dedup_images: Avg: 1.02 ms, Median: 1.02 ms, Total Time: 1.02 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:dedup_images_channel_in: Avg: 1.44 ms, Median: 1.44 ms, Total Time: 1.44 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor: Avg: 0.66 ms, Median: 0.66 ms, Total Time: 0.66 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:docx_content_extractor_channel_in: Avg: 1.09 ms, Median: 1.09 ms, Total Time: 1.09 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:filter_images: Avg: 0.84 ms, Median: 0.84 ms, Total Time: 0.84 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:filter_images_channel_in: Avg: 7.75 ms, Median: 7.75 ms, Total Time: 7.75 ms, Total % of Trace Computation: 0.07%\nINFO:nv_ingest_client.cli.util.processing:job_counter: Avg: 2.13 ms, Median: 2.13 ms, Total Time: 2.13 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:job_counter_channel_in: Avg: 2.05 ms, Median: 2.05 ms, Total Time: 2.05 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection: Avg: 14.48 ms, Median: 14.48 ms, Total Time: 14.48 ms, Total % of Trace Computation: 0.14%\nINFO:nv_ingest_client.cli.util.processing:metadata_injection_channel_in: Avg: 0.22 ms, Median: 0.22 ms, Total Time: 0.22 ms, Total % of Trace Computation: 0.00%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor: Avg: 10332.97 ms, Median: 10332.97 ms, Total Time: 10332.97 ms, Total % of Trace Computation: 99.45%\nINFO:nv_ingest_client.cli.util.processing:pdf_content_extractor_channel_in: Avg: 0.44 ms, Median: 0.44 ms, Total Time: 0.44 ms, Total % of Trace Computation: 0.00%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor: Avg: 1.19 ms, Median: 1.19 ms, Total Time: 1.19 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:pptx_content_extractor_channel_in: Avg: 0.98 ms, Median: 0.98 ms, Total Time: 0.98 ms, Total % of Trace Computation: 0.01%\nINFO:nv_ingest_client.cli.util.processing:redis_source_network_in: Avg: 12.27 ms, Median: 12.27 ms, Total Time: 12.27 ms, Total % of Trace Computation: 0.12%\nINFO:nv_ingest_client.cli.util.processing:redis_task_sink_channel_in: Avg: 2.16 ms, Median: 2.16 ms, Total Time: 2.16 ms, Total % of Trace Computation: 0.02%\nINFO:nv_ingest_client.cli.util.processing:redis_task_source: Avg: 8.00 ms, Median: 8.00 ms, Total Time: 8.00 ms, Total % of Trace Computation: 0.08%\nINFO:nv_ingest_client.cli.util.processing:Unresolved time: 82.82 ms, Percent of Total Elapsed: 0.79%\nINFO:nv_ingest_client.cli.util.processing:Processed 1 files in 10.47 seconds.\nINFO:nv_ingest_client.cli.util.processing:Total pages processed: 3\nINFO:nv_ingest_client.cli.util.processing:Throughput (Pages/sec): 0.29\nINFO:nv_ingest_client.cli.util.processing:Throughput (Files/sec): 0.10\n</code></pre></p>"},{"location":"user-guide/getting-started/quickstart-guide/#step-4-inspecting-and-consuming-results","title":"Step 4: Inspecting and Consuming Results","text":"<p>After the ingestion steps above have been completed, you should be able to find the <code>text</code> and <code>image</code> subfolders inside your processed docs folder. Each will contain JSON-formatted extracted content and metadata.</p> <p>When processing has completed, you'll have separate result files for text and image data: <pre><code>ls -R processed_docs/\n</code></pre> <pre><code>processed_docs/:\nimage  structured  text\n\nprocessed_docs/image:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/structured:\nmultimodal_test.pdf.metadata.json\n\nprocessed_docs/text:\nmultimodal_test.pdf.metadata.json\n</code></pre> You can view the full JSON extracts and the metadata definitions here. We also provide a script for inspecting extracted images.</p> <p>First, install <code>tkinter</code> by running the following commands depending on your OS. - For Ubuntu/Debian Linux: <pre><code>sudo apt-get update\nsudo apt-get install python3-tk\n</code></pre> - For Fedora/RHEL Linux: <pre><code>sudo dnf install python3-tkinter\n</code></pre> - For macOS using Homebrew: <pre><code>brew install python-tk\n</code></pre> Then, run the following command to execute the script for inspecting the extracted image: <pre><code>python src/util/image_viewer.py --file_path ./processed_docs/image/multimodal_test.pdf.metadata.json\n</code></pre></p> <p>TIP: Beyond inspecting the results, you can read them into things like llama-index or langchain retrieval pipelines.</p> <p>Also, checkout our demo using a retrieval pipeline on build.nvidia.com to query over document content pre-extracted with NV-Ingest.</p>"},{"location":"user-guide/getting-started/quickstart-guide/#repo-structure","title":"Repo Structure","text":"<p>Beyond the relevant documentation, examples, and other links above, below is a description of the contents in this repo's folders:</p> <ol> <li>.github: GitHub repo configuration files</li> <li>ci: Scripts used to build the NV-Ingest container and other packages</li> <li>client: Docs and source code for the nv-ingest-cli utility</li> <li>config: Various .yaml files defining configuration for OTEL, Prometheus</li> <li>data: Sample PDFs provided for testing convenience</li> <li>docker: Houses scripts used by the nv-ingest docker container</li> <li>docs: Various READMEs describing deployment, metadata schemas, auth and telemetry setup</li> <li>examples: Example notebooks, scripts, and longer-form tutorial content</li> <li>helm: Documentation for deploying NV-Ingest to a Kubernetes cluster via Helm chart</li> <li>skaffold: Skaffold configuration</li> <li>src: Source code for the NV-Ingest pipelines and service</li> <li>tests: Unit tests for NV-Ingest</li> </ol>"},{"location":"user-guide/getting-started/quickstart-guide/#notices","title":"Notices","text":""},{"location":"user-guide/getting-started/quickstart-guide/#third-party-license-notice","title":"Third-Party License Notice:","text":"<p>If configured to do so, this project will download and install additional third-party open-source software projects. Review the license terms of these open-source projects before use:</p> <p>https://pypi.org/project/pdfservices-sdk/</p> <ul> <li><code>INSTALL_ADOBE_SDK</code>:</li> <li>Description: If set to <code>true</code>, the Adobe SDK will be installed in the container at launch time. This is     required if you want to use the Adobe extraction service for PDF decomposition. Review the     license agreement for the     pdfservices-sdk before enabling this option.</li> </ul>"},{"location":"user-guide/getting-started/quickstart-guide/#contributing","title":"Contributing","text":"<p>We require that all contributors \"sign off\" on their commits. This certifies that the contribution is your original work, or you have rights to submit it under the same license or a compatible license.</p> <p>Any contribution that contains commits that aren't signed off won't be accepted.</p> <p>To sign off on a commit, use the --signoff (or -s) option when committing your changes:</p> <pre><code>$ git commit -s -m \"Add cool feature.\"\n</code></pre> <p>This appends the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre>"},{"location":"user-guide/getting-started/quickstart-guide/#full-text-of-the-dco","title":"Full text of the DCO:","text":"<pre><code>  Developer Certificate of Origin\n  Version 1.1\n\n  Copyright (C) 2004, 2006 The Linux Foundation and its contributors.\n  1 Letterman Drive\n  Suite D4700\n  San Francisco, CA, 94129\n\n  Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n</code></pre> <pre><code>  Developer's Certificate of Origin 1.1\n\n  By making a contribution to this project, I certify that:\n\n  (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n  (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open-source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n  (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n  (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"}]}